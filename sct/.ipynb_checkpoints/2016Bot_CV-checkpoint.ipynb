{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2016 Bot \n",
    "\n",
    "### Overview\n",
    "- Introduction \n",
    "- Data ETL\n",
    "- Data Augmentation \n",
    "- Networks Architecture \n",
    "- Training \n",
    "- Model Averaging \n",
    "- Conclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.io import imread\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Augmentation \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# model build \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conf:\n",
    "    def __init__(self, confPath):\n",
    "        # load and store the configuration and update the object's dictionary\n",
    "        conf = json.loads(open(confPath).read())\n",
    "        self.__dict__.update(conf)\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        # return the value associated with the supplied key\n",
    "        return self.__dict__.get(k, None)\n",
    "    \n",
    "    \n",
    "def auto_resized(img,size):\n",
    "    '''size = (width,height)'''\n",
    "    size = tuple(size)\n",
    "    resize_img = cv2.resize(img, size, interpolation=cv2.INTER_LINEAR)\n",
    "    return resize_img\n",
    "\n",
    "def TrainFilePath(folderPath, constrain=None, **kargs):\n",
    "    '''\n",
    "    (1) Output filepath and calssName\n",
    "    (2) folderPath \n",
    "          --label_1\n",
    "           -- xxx.jpg\n",
    "    '''\n",
    "    assert folderPath[-1]!='/'\n",
    "    if constrain is None:\n",
    "        constrain = ('avi', 'mp4','png','jpg') \n",
    "    for (rootDir, dirNames, fileNames) in os.walk(folderPath):\n",
    "        for fileName in fileNames:\n",
    "            if fileName.split('.')[-1] in constrain:\n",
    "                yield (os.path.join(rootDir, fileName)) \n",
    "                \n",
    "#img_channels = 3\n",
    "def genTrX(filePath, resolution, img_channels=3):\n",
    "    assert type(resolution) == tuple\n",
    "    img = auto_resized(imread(filePath),resolution)  #conf['sliding_size']\n",
    "    if img_channels==1:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    elif img_channels==3:\n",
    "        img = img[:,:,:3]\n",
    "    return img\n",
    "                \n",
    "def load_training(folderList, img_rows, img_cols, img_channels):\n",
    "    TrY = []\n",
    "    TrX = []\n",
    "    TrY_template = np.eye(len(folderList))\n",
    "    for eyeId, folderPath in enumerate(folderList):\n",
    "        for imgPath in TrainFilePath(folderPath) :\n",
    "            TrY.append(TrY_template[eyeId])\n",
    "            TrX.append(genTrX(imgPath, (img_rows,img_cols), img_channels))\n",
    "    print (len(TrX))\n",
    "    return TrX, TrY\n",
    "\n",
    "def create_folderList(rootDir):\n",
    "    result=[]\n",
    "    for a in os.listdir(rootDir):\n",
    "        a = os.path.join(rootDir, a)\n",
    "        if os.path.isdir(a):\n",
    "            result.append(a) \n",
    "    return result\n",
    "\n",
    "\n",
    "def reshapeShuffle(TrX, TrY, img_rows, img_cols, img_channels):\n",
    "    trainX = np.asarray(TrX, dtype = np.uint8)\n",
    "    trainX = trainX.reshape(trainX.shape[0], img_channels, img_rows, img_cols)\n",
    "    trainX = trainX.astype('float32')\n",
    "    trainY = np.asarray(TrY, dtype = np.float32)\n",
    "    # shuffle\n",
    "    trainX , trainY = shuffle(trainX,trainY)\n",
    "    print ('Train_X : ',trainX.shape,'Train_Y' ,trainY.shape)\n",
    "    return trainX , trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116481\n"
     ]
    }
   ],
   "source": [
    "ROOT_Dir = 'D:\\\\2016bot_cv'\n",
    "\n",
    "img_rows= 48\n",
    "\n",
    "img_cols= 48\n",
    "\n",
    "img_channels=3\n",
    "\n",
    "folderList = create_folderList(ROOT_Dir)\n",
    "\n",
    "Train_X, Train_Y = load_training(folderList, img_rows, img_cols, img_channels)\n",
    "\n",
    "\n",
    "\n",
    "#TrainFilePath('D:\\2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train_X : ', (116481L, 3L, 48L, 48L), 'Train_Y', (116481L, 12L))\n"
     ]
    }
   ],
   "source": [
    "train_X , train_Y = reshapeShuffle(Train_X, Train_Y, img_rows, img_cols, img_channels=img_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_Img = ImageDataGenerator(featurewise_center=False,\n",
    "    samplewise_center=True,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=True,\n",
    "    rotation_range=180.,\n",
    "    width_shift_range=.2,\n",
    "    height_shift_range=.2,\n",
    "    shear_range=.2,\n",
    "    zoom_range=[0.5, 1.5],\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_Img.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class ConvNetFactory:\n",
    "\tdef __init__(self):\n",
    "\t\tpass\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef build(name, *args, **kargs):\n",
    "\t\t# define the network (i.e., string => function) mappings\n",
    "\t\tmappings = {\n",
    "\t\t\t\"shallownet\": ConvNetFactory.ShallowNet,\n",
    "\t\t\t\"lenet\": ConvNetFactory.LeNet,\n",
    "\t\t\t\"karpathynet\": ConvNetFactory.KarpathyNet,\n",
    "\t\t\t\"minivggnet\": ConvNetFactory.MiniVGGNet}\n",
    "\n",
    "\t\t# grab the builder function from the mappings dictionary\n",
    "\t\tbuilder = mappings.get(name, None)\n",
    "\n",
    "\t\t# if the builder is None, then there is not a function that can be used\n",
    "\t\t# to build to the network, so return None\n",
    "\t\tif builder is None:\n",
    "\t\t\treturn None\n",
    "\n",
    "\t\t# otherwise, build the network architecture\n",
    "\t\treturn builder(*args, **kargs)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef ShallowNet(numChannels, imgRows, imgCols, numClasses, **kwargs):\n",
    "\t\t# initialzie the model\n",
    "\t\tmodel = Sequential()\n",
    "\n",
    "\t\t# define the first (and only) CONV => RELU layer\n",
    "\t\tmodel.add(Convolution2D(32, 3, 3, border_mode=\"same\",\n",
    "\t\t\tinput_shape=(numChannels, imgRows, imgCols)))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\n",
    "\t\t# add a FC layer followed by the soft-max classifier\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(numClasses))\n",
    "\t\tmodel.add(Activation(\"softmax\"))\n",
    "\n",
    "\t\t# return the network architecture\n",
    "\t\treturn model\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef LeNet(numChannels, imgRows, imgCols, numClasses, activation=\"tanh\", **kwargs):\n",
    "\t\t# initialize the model\n",
    "\t\tmodel = Sequential()\n",
    "\n",
    "\t\t# define the first set of CONV => ACTIVATION => POOL layers\n",
    "\t\tmodel.add(Convolution2D(20, 5, 5, border_mode=\"same\",\n",
    "\t\t\tinput_shape=(numChannels, imgRows, imgCols)))\n",
    "\t\tmodel.add(Activation(activation))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\t\t# define the second set of CONV => ACTIVATION => POOL layers\n",
    "\t\tmodel.add(Convolution2D(50, 5, 5, border_mode=\"same\"))\n",
    "\t\tmodel.add(Activation(activation))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\t\t# define the first FC => ACTIVATION layers\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(500))\n",
    "\t\tmodel.add(Activation(activation))\n",
    "\n",
    "\t\t# define the second FC layer\n",
    "\t\tmodel.add(Dense(numClasses))\n",
    "\n",
    "\t\t# lastly, define the soft-max classifier\n",
    "\t\tmodel.add(Activation(\"softmax\"))\n",
    "\n",
    "\t\t# return the network architecture\n",
    "\t\treturn model\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef KarpathyNet():\n",
    "\t\t# initialize the model\n",
    "\t\tmodel = Sequential()\n",
    "\n",
    "\t\t# define the first set of CONV => RELU => POOL layers\n",
    "\t\tmodel.add(Convolution2D(16, 5, 5, border_mode=\"same\",\n",
    "\t\t\tinput_shape=(numChannels, imgRows, imgCols)))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\t\t# check to see if dropout should be applied to reduce overfitting\n",
    "\t\tif dropout:\n",
    "\t\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# define the second set of CONV => RELU => POOL layers\n",
    "\t\tmodel.add(Convolution2D(20, 5, 5, border_mode=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\t\t# check to see if dropout should be applied to reduce overfitting\n",
    "\t\tif dropout:\n",
    "\t\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# define the third set of CONV => RELU => POOL layers\n",
    "\t\tmodel.add(Convolution2D(20, 5, 5, border_mode=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\t\t# check to see if dropout should be applied to reduce overfitting\n",
    "\t\tif dropout:\n",
    "\t\t\tmodel.add(Dropout(0.5))\n",
    "\n",
    "\t\t# define the soft-max classifier\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(numClasses))\n",
    "\t\tmodel.add(Activation(\"softmax\"))\n",
    "\n",
    "\t\t# return the network architecture\n",
    "\t\treturn model\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef MiniVGGNet():\n",
    "\t\t# initialize the model\n",
    "\t\tmodel = Sequential()\n",
    "\n",
    "\t\t# define the first set of CONV => RELU => CONV => RELU => POOL layers\n",
    "\t\tmodel.add(Convolution2D(32, 3, 3, border_mode=\"same\",\n",
    "\t\t\tinput_shape=(numChannels, imgRows, imgCols)))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(Convolution2D(32, 3, 3))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\t\t# check to see if dropout should be applied to reduce overfitting\n",
    "\t\tif dropout:\n",
    "\t\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# define the second set of CONV => RELU => CONV => RELU => POOL layers\n",
    "\t\tmodel.add(Convolution2D(64, 3, 3, border_mode=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(Convolution2D(64, 3, 3))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\t\t# check to see if dropout should be applied to reduce overfitting\n",
    "\t\tif dropout:\n",
    "\t\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# define the set of FC => RELU layers\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(512))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\n",
    "\t\t# check to see if dropout should be applied to reduce overfitting\n",
    "\t\tif dropout:\n",
    "\t\t\tmodel.add(Dropout(0.5))\n",
    "\n",
    "\t\t# define the soft-max classifier\n",
    "\t\tmodel.add(Dense(numClasses))\n",
    "\t\tmodel.add(Activation(\"softmax\"))\n",
    "\n",
    "\t\t# return the network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def VGG_19(weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,48,48)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(12, activation='softmax'))\n",
    "    \n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "zeropadding2d_81 (ZeroPadding2D) (None, 3, 50, 50)     0           zeropadding2d_input_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_81 (Convolution2D) (None, 64, 48, 48)    1792        zeropadding2d_81[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_82 (ZeroPadding2D) (None, 64, 50, 50)    0           convolution2d_81[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_82 (Convolution2D) (None, 64, 48, 48)    36928       zeropadding2d_82[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_26 (MaxPooling2D)   (None, 64, 24, 24)    0           convolution2d_82[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_83 (ZeroPadding2D) (None, 64, 26, 26)    0           maxpooling2d_26[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_83 (Convolution2D) (None, 128, 24, 24)   73856       zeropadding2d_83[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_84 (ZeroPadding2D) (None, 128, 26, 26)   0           convolution2d_83[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_84 (Convolution2D) (None, 128, 24, 24)   147584      zeropadding2d_84[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_27 (MaxPooling2D)   (None, 128, 12, 12)   0           convolution2d_84[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_85 (ZeroPadding2D) (None, 128, 14, 14)   0           maxpooling2d_27[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_85 (Convolution2D) (None, 256, 12, 12)   295168      zeropadding2d_85[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_86 (ZeroPadding2D) (None, 256, 14, 14)   0           convolution2d_85[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_86 (Convolution2D) (None, 256, 12, 12)   590080      zeropadding2d_86[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_87 (ZeroPadding2D) (None, 256, 14, 14)   0           convolution2d_86[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_87 (Convolution2D) (None, 256, 12, 12)   590080      zeropadding2d_87[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_88 (ZeroPadding2D) (None, 256, 14, 14)   0           convolution2d_87[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_88 (Convolution2D) (None, 256, 12, 12)   590080      zeropadding2d_88[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_28 (MaxPooling2D)   (None, 256, 6, 6)     0           convolution2d_88[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_89 (ZeroPadding2D) (None, 256, 8, 8)     0           maxpooling2d_28[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_89 (Convolution2D) (None, 512, 6, 6)     1180160     zeropadding2d_89[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_90 (ZeroPadding2D) (None, 512, 8, 8)     0           convolution2d_89[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_90 (Convolution2D) (None, 512, 6, 6)     2359808     zeropadding2d_90[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_91 (ZeroPadding2D) (None, 512, 8, 8)     0           convolution2d_90[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_91 (Convolution2D) (None, 512, 6, 6)     2359808     zeropadding2d_91[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_92 (ZeroPadding2D) (None, 512, 8, 8)     0           convolution2d_91[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_92 (Convolution2D) (None, 512, 6, 6)     2359808     zeropadding2d_92[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_29 (MaxPooling2D)   (None, 512, 3, 3)     0           convolution2d_92[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_93 (ZeroPadding2D) (None, 512, 5, 5)     0           maxpooling2d_29[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_93 (Convolution2D) (None, 512, 3, 3)     2359808     zeropadding2d_93[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_94 (ZeroPadding2D) (None, 512, 5, 5)     0           convolution2d_93[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_94 (Convolution2D) (None, 512, 3, 3)     2359808     zeropadding2d_94[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_95 (ZeroPadding2D) (None, 512, 5, 5)     0           convolution2d_94[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_95 (Convolution2D) (None, 512, 3, 3)     2359808     zeropadding2d_95[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_96 (ZeroPadding2D) (None, 512, 5, 5)     0           convolution2d_95[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_96 (Convolution2D) (None, 512, 3, 3)     2359808     zeropadding2d_96[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_30 (MaxPooling2D)   (None, 512, 1, 1)     0           convolution2d_96[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 512)           0           maxpooling2d_30[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 4096)          2101248     flatten_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 4096)          0           dense_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 2096)          8587312     dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 2096)          0           dense_17[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 500)           1048500     dropout_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 500)           0           dense_18[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 12)            6012        dropout_13[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 31767456\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = VGG_19()\n",
    "#https://gist.github.com/baraldilorenzo/8d096f48a1be4a2d660d\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: nvcc STDOUT mod.cu\r\n",
      "   Creating library C:/Users/kentc/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-2.7.12-64/tmp6x0vp4/f0a029f6487992dc0aaa50c01588b3d2.lib and object C:/Users/kentc/AppData/Local/Theano/compiledir_Windows-10-10.0.14393-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-2.7.12-64/tmp6x0vp4/f0a029f6487992dc0aaa50c01588b3d2.exp\r\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 240 samples, validate on 38827 samples\n",
      "Epoch 1/250\n",
      "240/240 [==============================] - 162s - loss: 2.5290 - val_loss: 2.5435\n",
      "Epoch 2/250\n",
      "240/240 [==============================] - 160s - loss: 2.5254 - val_loss: 2.5397\n",
      "Epoch 3/250\n",
      "240/240 [==============================] - 157s - loss: 2.5210 - val_loss: 2.5354\n",
      "Epoch 4/250\n",
      "240/240 [==============================] - 160s - loss: 2.5159 - val_loss: 2.5307\n",
      "Epoch 5/250\n",
      "240/240 [==============================] - 163s - loss: 2.5104 - val_loss: 2.5260\n",
      "Epoch 6/250\n",
      "240/240 [==============================] - 161s - loss: 2.5047 - val_loss: 2.5213\n",
      "Epoch 7/250\n",
      "240/240 [==============================] - 165s - loss: 2.4990 - val_loss: 2.5167\n",
      "Epoch 8/250\n",
      "240/240 [==============================] - 163s - loss: 2.4934 - val_loss: 2.5123\n",
      "Epoch 9/250\n",
      "240/240 [==============================] - 162s - loss: 2.4879 - val_loss: 2.5083\n",
      "Epoch 10/250\n",
      "240/240 [==============================] - 158s - loss: 2.4828 - val_loss: 2.5046\n",
      "Epoch 11/250\n",
      "240/240 [==============================] - 158s - loss: 2.4780 - val_loss: 2.5013\n",
      "Epoch 12/250\n",
      "240/240 [==============================] - 158s - loss: 2.4735 - val_loss: 2.4983\n",
      "Epoch 13/250\n",
      "240/240 [==============================] - 158s - loss: 2.4695 - val_loss: 2.4957\n",
      "Epoch 14/250\n",
      "240/240 [==============================] - 157s - loss: 2.4658 - val_loss: 2.4935\n",
      "Epoch 15/250\n",
      "240/240 [==============================] - 158s - loss: 2.4626 - val_loss: 2.4916\n",
      "Epoch 16/250\n",
      "240/240 [==============================] - 158s - loss: 2.4597 - val_loss: 2.4900\n",
      "Epoch 17/250\n",
      "240/240 [==============================] - 158s - loss: 2.4571 - val_loss: 2.4887\n",
      "Epoch 18/250\n",
      "240/240 [==============================] - 159s - loss: 2.4549 - val_loss: 2.4876\n",
      "Epoch 19/250\n",
      "240/240 [==============================] - 158s - loss: 2.4529 - val_loss: 2.4868\n",
      "Epoch 20/250\n",
      "240/240 [==============================] - 158s - loss: 2.4512 - val_loss: 2.4861\n",
      "Epoch 21/250\n",
      "240/240 [==============================] - 158s - loss: 2.4498 - val_loss: 2.4856\n",
      "Epoch 22/250\n",
      "240/240 [==============================] - 157s - loss: 2.4485 - val_loss: 2.4853\n",
      "Epoch 23/250\n",
      "240/240 [==============================] - 157s - loss: 2.4475 - val_loss: 2.4850\n",
      "Epoch 24/250\n",
      "240/240 [==============================] - 157s - loss: 2.4466 - val_loss: 2.4849\n",
      "Epoch 25/250\n",
      "240/240 [==============================] - 157s - loss: 2.4458 - val_loss: 2.4848\n",
      "Epoch 26/250\n",
      "240/240 [==============================] - 158s - loss: 2.4452 - val_loss: 2.4849\n",
      "Epoch 27/250\n",
      "240/240 [==============================] - 157s - loss: 2.4447 - val_loss: 2.4849\n",
      "Epoch 28/250\n",
      "240/240 [==============================] - 157s - loss: 2.4442 - val_loss: 2.4850\n",
      "Epoch 29/250\n",
      "240/240 [==============================] - 157s - loss: 2.4438 - val_loss: 2.4851\n",
      "Epoch 30/250\n",
      "240/240 [==============================] - 159s - loss: 2.4435 - val_loss: 2.4853\n",
      "Epoch 31/250\n",
      "240/240 [==============================] - 157s - loss: 2.4433 - val_loss: 2.4854\n",
      "Epoch 32/250\n",
      "240/240 [==============================] - 157s - loss: 2.4431 - val_loss: 2.4856\n",
      "Epoch 33/250\n",
      "240/240 [==============================] - 158s - loss: 2.4429 - val_loss: 2.4857\n",
      "Epoch 34/250\n",
      "240/240 [==============================] - 158s - loss: 2.4427 - val_loss: 2.4859\n",
      "Epoch 35/250\n",
      "240/240 [==============================] - 159s - loss: 2.4426 - val_loss: 2.4860\n",
      "Epoch 36/250\n",
      "240/240 [==============================] - 158s - loss: 2.4425 - val_loss: 2.4862\n",
      "Epoch 37/250\n",
      "240/240 [==============================] - 157s - loss: 2.4424 - val_loss: 2.4863\n",
      "Epoch 38/250\n",
      "240/240 [==============================] - 157s - loss: 2.4424 - val_loss: 2.4864\n",
      "Epoch 39/250\n",
      "240/240 [==============================] - 157s - loss: 2.4423 - val_loss: 2.4865\n",
      "Epoch 40/250\n",
      "240/240 [==============================] - 157s - loss: 2.4422 - val_loss: 2.4866\n",
      "Epoch 41/250\n",
      "240/240 [==============================] - 158s - loss: 2.4422 - val_loss: 2.4867\n",
      "Epoch 42/250\n",
      "240/240 [==============================] - 157s - loss: 2.4421 - val_loss: 2.4868\n",
      "Epoch 43/250\n",
      "240/240 [==============================] - 157s - loss: 2.4421 - val_loss: 2.4868\n",
      "Epoch 44/250\n",
      "240/240 [==============================] - 157s - loss: 2.4421 - val_loss: 2.4869\n",
      "Epoch 45/250\n",
      "240/240 [==============================] - 157s - loss: 2.4421 - val_loss: 2.4869\n",
      "Epoch 46/250\n",
      "240/240 [==============================] - 157s - loss: 2.4420 - val_loss: 2.4869\n",
      "Epoch 47/250\n",
      "240/240 [==============================] - 158s - loss: 2.4420 - val_loss: 2.4870\n",
      "Epoch 48/250\n",
      "240/240 [==============================] - 157s - loss: 2.4420 - val_loss: 2.4870\n",
      "Epoch 49/250\n",
      "240/240 [==============================] - 157s - loss: 2.4420 - val_loss: 2.4870\n",
      "Epoch 50/250\n",
      "240/240 [==============================] - 158s - loss: 2.4420 - val_loss: 2.4870\n",
      "Epoch 51/250\n",
      "240/240 [==============================] - 158s - loss: 2.4419 - val_loss: 2.4870\n",
      "Epoch 52/250\n",
      "240/240 [==============================] - 159s - loss: 2.4419 - val_loss: 2.4869\n",
      "Epoch 53/250\n",
      "240/240 [==============================] - 158s - loss: 2.4419 - val_loss: 2.4869\n",
      "Epoch 54/250\n",
      "240/240 [==============================] - 158s - loss: 2.4419 - val_loss: 2.4869\n",
      "Epoch 55/250\n",
      "240/240 [==============================] - 157s - loss: 2.4419 - val_loss: 2.4868\n",
      "Epoch 56/250\n",
      "240/240 [==============================] - 157s - loss: 2.4419 - val_loss: 2.4868\n",
      "Epoch 57/250\n",
      "240/240 [==============================] - 157s - loss: 2.4418 - val_loss: 2.4867\n",
      "Epoch 58/250\n",
      "240/240 [==============================] - 158s - loss: 2.4418 - val_loss: 2.4867\n",
      "Epoch 59/250\n",
      "240/240 [==============================] - 157s - loss: 2.4418 - val_loss: 2.4866\n",
      "Epoch 60/250\n",
      "240/240 [==============================] - 157s - loss: 2.4418 - val_loss: 2.4866\n",
      "Epoch 61/250\n",
      "240/240 [==============================] - 157s - loss: 2.4418 - val_loss: 2.4865\n",
      "Epoch 62/250\n",
      "240/240 [==============================] - 157s - loss: 2.4418 - val_loss: 2.4865\n",
      "Epoch 63/250\n",
      "240/240 [==============================] - 157s - loss: 2.4418 - val_loss: 2.4864\n",
      "Epoch 64/250\n",
      "240/240 [==============================] - 159s - loss: 2.4418 - val_loss: 2.4863\n",
      "Epoch 65/250\n",
      "240/240 [==============================] - 157s - loss: 2.4418 - val_loss: 2.4863\n",
      "Epoch 66/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4862\n",
      "Epoch 67/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4862\n",
      "Epoch 68/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4861\n",
      "Epoch 69/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4861\n",
      "Epoch 70/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4860\n",
      "Epoch 71/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4860\n",
      "Epoch 72/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4859\n",
      "Epoch 73/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4859\n",
      "Epoch 74/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4858\n",
      "Epoch 75/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4858\n",
      "Epoch 76/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4857\n",
      "Epoch 77/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4857\n",
      "Epoch 78/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4856\n",
      "Epoch 79/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4856\n",
      "Epoch 80/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4856\n",
      "Epoch 81/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4856\n",
      "Epoch 82/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4855\n",
      "Epoch 83/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4855\n",
      "Epoch 84/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4855\n",
      "Epoch 85/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4855\n",
      "Epoch 86/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 87/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 88/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 89/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 90/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 91/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 92/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 93/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 94/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4853\n",
      "Epoch 95/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4853\n",
      "Epoch 96/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4853\n",
      "Epoch 97/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4853\n",
      "Epoch 98/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4853\n",
      "Epoch 99/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4853\n",
      "Epoch 100/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4853\n",
      "Epoch 101/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4853\n",
      "Epoch 102/250\n",
      "240/240 [==============================] - 159s - loss: 2.4417 - val_loss: 2.4853\n",
      "Epoch 103/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4853\n",
      "Epoch 104/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4853\n",
      "Epoch 105/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4853\n",
      "Epoch 106/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4853\n",
      "Epoch 107/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4853\n",
      "Epoch 108/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 109/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 110/250\n",
      "240/240 [==============================] - 159s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 111/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 112/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 113/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 114/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 115/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 116/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 117/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 118/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 119/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 120/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 121/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 122/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 123/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 124/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 125/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 126/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 127/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 128/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 129/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 130/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 131/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 132/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 133/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 134/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 135/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 136/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 137/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 138/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 139/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 140/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 141/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 142/250\n",
      "240/240 [==============================] - 160s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 143/250\n",
      "240/240 [==============================] - 161s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 144/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 145/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 146/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 147/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 148/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 149/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 150/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 151/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 152/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 153/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 154/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 155/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 156/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 157/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 158/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 159/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 160/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 161/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 162/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 163/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 164/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 165/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 166/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 167/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 168/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 169/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 170/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 171/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 172/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 173/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 174/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 175/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 176/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 177/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 178/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 179/250\n",
      "240/240 [==============================] - 159s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 180/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 181/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 182/250\n",
      "240/240 [==============================] - 159s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 183/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 184/250\n",
      "240/240 [==============================] - 160s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 185/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 186/250\n",
      "240/240 [==============================] - 159s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 187/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 188/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 189/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 190/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 191/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 192/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 193/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 194/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 195/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 196/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 197/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 198/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 199/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 200/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 201/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 202/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 203/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 204/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 205/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 206/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 207/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 208/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 209/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 210/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 211/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 212/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 213/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 214/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 215/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 216/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 217/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 218/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 219/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 220/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 221/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 222/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 223/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 224/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 225/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 226/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 227/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 228/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 229/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 230/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 231/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 232/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 233/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 234/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 235/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 236/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 237/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 238/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 239/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 240/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 241/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 242/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 243/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 244/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 245/250\n",
      "240/240 [==============================] - 158s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 246/250\n",
      "240/240 [==============================] - 157s - loss: 2.4417 - val_loss: 2.4854\n",
      "Epoch 247/250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-c340dc02af1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mTe_Y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen_Img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTr_X\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mTr_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m240\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m240\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTe_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTe_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\kentc\\Anaconda2\\lib\\site-packages\\keras\\models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m                               sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Users\\kentc\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m   1080\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kentc\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[0;32m    813\u001b[0m                         val_outs = self._test_loop(val_f, val_ins,\n\u001b[0;32m    814\u001b[0m                                                    \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 815\u001b[1;33m                                                    verbose=0)\n\u001b[0m\u001b[0;32m    816\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m                             \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kentc\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36m_test_loop\u001b[1;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[0;32m    898\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 900\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    901\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kentc\\Anaconda2\\lib\\site-packages\\keras\\backend\\theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kentc\\Anaconda2\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "kf = KFold(len(train_Y), n_folds=3)\n",
    "for train, test in kf:\n",
    "    #print (train, test)\n",
    "    Tr_X = train_X[train]\n",
    "    Te_X = train_X[test]\n",
    "    Tr_Y = train_Y[train]\n",
    "    Te_Y = train_Y[test]\n",
    "    for X_batch, y_batch in gen_Img.flow(Tr_X , Tr_Y, batch_size=240):\n",
    "        model.fit(X_batch, y_batch, 240, nb_epoch=250,verbose=1, validation_data=(Te_X, Te_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-710c35ee8e99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTe_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTe_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "score = model.evaluate(Te_X, Te_Y, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.12500012,  0.08750004,  0.09999988,  0.0791666 ,  0.09166677,\n",
       "         0.12499985,  0.07083347,  0.05833334,  0.06666683,  0.06249999,\n",
       "         0.04166663,  0.09166655]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(Te_X[3].reshape(1,img_channels,img_rows,img_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image Augmentation for Deep Learning With Keras\n",
    "by Jason Brownlee on June 29, 2016 in Deep Learning\n",
    "0\n",
    "0\n",
    "5\n",
    "42\n",
    "Data preparation is required when working with neural network and deep learning models. Increasingly data augmentation is also required on more complex object recognition tasks.\n",
    "\n",
    "In this post you will discover how to use data preparation and data augmentation with your image datasets when developing and evaluating deep learning models in Python with Keras.\n",
    "\n",
    "After reading this post, you will know:\n",
    "\n",
    "About the image augmentation API provide by Keras and how to use it with your models.\n",
    "How to perform feature standardization.\n",
    "How to perform ZCA whitening of your images.\n",
    "How to augment data with random rotations, shifts and flips.\n",
    "How to save augmented image data to disk.\n",
    "Let’s get started.\n",
    "\n",
    "Update: The examples in this post were updated for the latest Keras API. The datagen.next() function was removed.\n",
    "\n",
    "Keras Image Augmentation API\n",
    "\n",
    "Like the rest of Keras, the image augmentation API is simple and powerful.\n",
    "\n",
    "Keras provides the ImageDataGenerator class that defines the configuration for image data preparation and augmentation. This includes capabilities such as:\n",
    "\n",
    "Sample-wise standardization.\n",
    "Feature-wise standardization.\n",
    "ZCA whitening.\n",
    "Random rotation, shifts, shear and flips.\n",
    "Dimension reordering.\n",
    "Save augmented images to disk.\n",
    "An augmented image generator can be created as follows:\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator()\n",
    "1\n",
    "datagen = ImageDataGenerator()\n",
    "Rather than performing the operations on your entire image dataset in memory, the API is designed to be iterated by the deep learning model fitting process, creating augmented image data for you just-in-time. This reduces your memory overhead, but adds some additional time cost during model training.\n",
    "\n",
    "After you have created and configured your ImageDataGenerator, you must fit it on your data. This will calculate any statistics required to actually perform the transforms to your image data. You can do this by calling the fit() function on the data generator and pass it your training dataset.\n",
    "\n",
    "\n",
    "datagen.fit(train)\n",
    "1\n",
    "datagen.fit(train)\n",
    "The data generator itself is in fact an iterator, returning batches of image samples when requested. We can configure the batch size and prepare the data generator and get batches of images by calling the flow() function.\n",
    "\n",
    "\n",
    "X_batch, y_batch = datagen.flow(train, train, batch_size=32)\n",
    "1\n",
    "X_batch, y_batch = datagen.flow(train, train, batch_size=32)\n",
    "Finally we can make use of the data generator. Instead of calling the fit() function on our model, we must call the fit_generator() function and pass in the data generator and the desired length of an epoch as well as the total number of epochs on which to train.\n",
    "\n",
    "\n",
    "fit_generator(datagen, samples_per_epoch=len(train), nb_epoch=100)\n",
    "1\n",
    "fit_generator(datagen, samples_per_epoch=len(train), nb_epoch=100)\n",
    "You can learn more about the Keras image data generator API in the Keras documentation.\n",
    "\n",
    "Get Started in Deep Learning With Python\n",
    "\n",
    "Deep Learning with Python Mini-Course\n",
    "\n",
    "Deep Learning gets state-of-the-art results and Python hosts the most powerful tools. \n",
    "Get started now!\n",
    "\n",
    "PDF Download and Email Course.\n",
    "\n",
    "FREE 14-Day Mini-Course on \n",
    "Deep Learning With Python\n",
    "\n",
    "Download Your FREE Mini-Course\n",
    " \n",
    "\n",
    " Download your PDF containing all 14 lessons.\n",
    "\n",
    "Get your daily lesson via email with tips and tricks.\n",
    "\n",
    "Point of Comparison for Image Augmentation\n",
    "\n",
    "Now that you know how the image augmentation API in Keras works, let’s look at some examples.\n",
    "\n",
    "We will use the MNIST handwritten digit recognition task in these examples. To begin with, let’s take a look at the first 9 images in the training dataset.\n",
    "\n",
    "\n",
    "# Plot images\n",
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# create a grid of 3x3 images\n",
    "for i in range(0, 9):\n",
    "\tpyplot.subplot(330 + 1 + i)\n",
    "\tpyplot.imshow(X_train[i], cmap=pyplot.get_cmap('gray'))\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "\n",
    "# Plot images\n",
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# create a grid of 3x3 images\n",
    "for i in range(0, 9):\n",
    "\tpyplot.subplot(330 + 1 + i)\n",
    "\tpyplot.imshow(X_train[i], cmap=pyplot.get_cmap('gray'))\n",
    "# show the plot\n",
    "pyplot.show()\n",
    "Running this example provides the following image that we can use as a point of comparison with the image preparation and augmentation in the examples below.\n",
    "\n",
    "Example MNIST images\n",
    "Example MNIST images\n",
    "Feature Standardization\n",
    "\n",
    "It is also possible to standardize pixel values across the entire dataset. This is called feature standardization and mirrors the type of standardization often performed for each column in a tabular dataset.\n",
    "\n",
    "You can perform feature standardization by setting the featurewise_center and featurewise_std_normalization arguments on the ImageDataGenerator class. These are in fact set to True by default and creating an instance of ImageDataGenerator with no arguments will have the same effect.\n",
    "\n",
    "\n",
    "# Standardize images across the dataset, mean=0, stdev=1\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "\t# create a grid of 3x3 images\n",
    "\tfor i in range(0, 9):\n",
    "\t\tpyplot.subplot(330 + 1 + i)\n",
    "\t\tpyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "\t# show the plot\n",
    "\tpyplot.show()\n",
    "\tbreak\n",
    "1\n",
    "2\n",
    "\n",
    "# Standardize images across the dataset, mean=0, stdev=1\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "\t# create a grid of 3x3 images\n",
    "\tfor i in range(0, 9):\n",
    "\t\tpyplot.subplot(330 + 1 + i)\n",
    "\t\tpyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "\t# show the plot\n",
    "\tpyplot.show()\n",
    "\tbreak\n",
    "Running this example you can see that the effect is different, seemingly darkening and lightening different digits.\n",
    "\n",
    "Standardized Feature MNIST Images\n",
    "Standardized Feature MNIST Images\n",
    "ZCA Whitening\n",
    "\n",
    "A whitening transform of an image is a linear algebra operation that reduces the redundancy in the matrix of pixel images.\n",
    "\n",
    "Less redundancy in the image is intended to better highlight the structures and features in the image to the learning algorithm.\n",
    "\n",
    "Typically, image whitening is performed using the Principal Component Analysis (PCA) technique. More recently, an alternative called ZCA (learn more in Appendix A of this tech report) shows better results and results in transformed images that keeps all of the original dimensions and unlike PCA, resulting transformed images still look like their originals.\n",
    "\n",
    "You can perform a ZCA whitening transform by setting the zca_whitening argument to True.\n",
    "\n",
    "\n",
    "# ZCA whitening\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator(zca_whitening=True)\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "\t# create a grid of 3x3 images\n",
    "\tfor i in range(0, 9):\n",
    "\t\tpyplot.subplot(330 + 1 + i)\n",
    "\t\tpyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "\t# show the plot\n",
    "\tpyplot.show()\n",
    "\tbreak\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "# ZCA whitening\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator(zca_whitening=True)\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "\t# create a grid of 3x3 images\n",
    "\tfor i in range(0, 9):\n",
    "\t\tpyplot.subplot(330 + 1 + i)\n",
    "\t\tpyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "\t# show the plot\n",
    "\tpyplot.show()\n",
    "\tbreak\n",
    "Running the example, you can see the same general structure in the images and how the outline of each digit has been highlighted.\n",
    "\n",
    "ZCA Whitening MNIST Images\n",
    "ZCA Whitening MNIST Images\n",
    "Random Rotations\n",
    "\n",
    "Sometimes images in your sample data may have varying and different rotations in the scene.\n",
    "\n",
    "You can train your model to better handle rotations of images by artificially and randomly rotating images from your dataset during training.\n",
    "\n",
    "The example below creates random rotations of the MNIST digits up to 90 degrees by setting the rotation_range argument.\n",
    "\n",
    "\n",
    "# Random Rotations\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator(rotation_range=90)\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "\t# create a grid of 3x3 images\n",
    "\tfor i in range(0, 9):\n",
    "\t\tpyplot.subplot(330 + 1 + i)\n",
    "\t\tpyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "\t# show the plot\n",
    "\tpyplot.show()\n",
    "\tbreak\n",
    "\n",
    "\n",
    "# Random Rotations\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator(rotation_range=90)\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "\t# create a grid of 3x3 images\n",
    "\tfor i in range(0, 9):\n",
    "\t\tpyplot.subplot(330 + 1 + i)\n",
    "\t\tpyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "\t# show the plot\n",
    "\tpyplot.show()\n",
    "\tbreak\n",
    "Running the example, you can see that images have been rotated left and right up to a limit of 90 degrees. This is not helpful on this problem because the MNIST digits have a normalized orientation, but this transform might be of help when learning from photographs where the objects may have different orientations.\n",
    "\n",
    "Random Rotations of MNIST Images\n",
    "Random Rotations of MNIST Images\n",
    "Random Shifts\n",
    "\n",
    "Objects in your images may not be centered in the frame. They may be off-center in a variety of different ways.\n",
    "\n",
    "You can train your deep learning network to expect and currently handle off-center objects by artificially creating shifted versions of your training data. Keras supports separate horizontal and vertical random shifting of training data by the width_shift_range and height_shift_range arguments.\n",
    "\n",
    "\n",
    "# Random Shifts\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# define data preparation\n",
    "shift = 0.2\n",
    "datagen = ImageDataGenerator(width_shift_range=shift, height_shift_range=shift)\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "\t# create a grid of 3x3 images\n",
    "\tfor i in range(0, 9):\n",
    "\t\tpyplot.subplot(330 + 1 + i)\n",
    "\t\tpyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "\t# show the plot\n",
    "\tpyplot.show()\n",
    "\tbreak\n",
    "1\n",
    "\n",
    "\n",
    "# Random Shifts\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# define data preparation\n",
    "shift = 0.2\n",
    "datagen = ImageDataGenerator(width_shift_range=shift, height_shift_range=shift)\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "\t# create a grid of 3x3 images\n",
    "\tfor i in range(0, 9):\n",
    "\t\tpyplot.subplot(330 + 1 + i)\n",
    "\t\tpyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "\t# show the plot\n",
    "\tpyplot.show()\n",
    "\tbreak\n",
    "Running this example creates shifted versions of the digits. Again, this is not required for MNIST as the handwritten digits are already centered, but you can see how this might be useful on more complex problem domains.\n",
    "\n",
    "Random Shifted MNIST Images\n",
    "Random Shifted MNIST Images\n",
    "Random Flips\n",
    "\n",
    "Another augmentation to your image data that can improve performance on large and complex problems is to create random flips of images in your training data.\n",
    "\n",
    "Keras supports random flipping along both the vertical and horizontal axes using the vertical_flip and horizontal_flip arguments.\n",
    "\n",
    "\n",
    "# Random Flips\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "\t# create a grid of 3x3 images\n",
    "\tfor i in range(0, 9):\n",
    "\t\tpyplot.subplot(330 + 1 + i)\n",
    "\t\tpyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "\t# show the plot\n",
    "\tpyplot.show()\n",
    "\tbreak\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "11\n",
    "12\n",
    "13\n",
    "14\n",
    "15\n",
    "16\n",
    "17\n",
    "18\n",
    "19\n",
    "20\n",
    "21\n",
    "22\n",
    "23\n",
    "24\n",
    "25\n",
    "# Random Flips\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "\t# create a grid of 3x3 images\n",
    "\tfor i in range(0, 9):\n",
    "\t\tpyplot.subplot(330 + 1 + i)\n",
    "\t\tpyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "\t# show the plot\n",
    "\tpyplot.show()\n",
    "\tbreak\n",
    "Running this example you can see flipped digits. Flipping digits is not useful as they will always have the correct left and right orientation, but this may be useful for problems with photographs of objects in a scene that can have a varied orientation.\n",
    "\n",
    "Randomly Flipped MNIST Images\n",
    "Randomly Flipped MNIST Images\n",
    "Saving Augmented Images to File\n",
    "\n",
    "The data preparation and augmentation is performed just in time by Keras.\n",
    "\n",
    "This is efficient in terms of memory, but you may require the exact images used during training. For example, perhaps you would like to use them with a different software package later or only generate them once and use them on multiple different deep learning models or configurations.\n",
    "\n",
    "Keras allows you to save the images generated during training. The directory, filename prefix and image file type can be specified to the flow() function before training. Then, during training, the generated images will be written to file.\n",
    "\n",
    "The example below demonstrates this and writes 9 images to a “images” subdirectory with the prefix “aug” and the file type of PNG.\n",
    "\n",
    "\n",
    "# Save augmented images to file\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator()\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "# configure batch size and retrieve one batch of images\n",
    "os.makedirs('images')\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9, save_to_dir='images', save_prefix='aug', save_format='png'):\n",
    "\t# create a grid of 3x3 images\n",
    "\tfor i in range(0, 9):\n",
    "\t\tpyplot.subplot(330 + 1 + i)\n",
    "\t\tpyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "\t# show the plot\n",
    "\tpyplot.show()\n",
    "\tbreak\n",
    "\n",
    "\n",
    "\n",
    "# Save augmented images to file\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator()\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "# configure batch size and retrieve one batch of images\n",
    "os.makedirs('images')\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9, save_to_dir='images', save_prefix='aug', save_format='png')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
