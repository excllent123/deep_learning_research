{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167v3.pdf)\n",
    "\n",
    "$$ \\text{By Kent Chiu @ 20161115} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Concept \n",
    "\n",
    "For me, the core concept is the **mapping ** & **difference.** \n",
    "\n",
    "First : Keep Difference is the thing that matters. \n",
    "Since the neuron network is just acting like the real neuron, the relative difference is the true thing that really matters. \n",
    "\n",
    "So basically, as long as we keep its relative difference, the neuron network could still works well. \n",
    "\n",
    "However, the actual amplitude, we could actually see it as the energy or voltage or whatever you like, just CPU nowadays could be drive in a lower votagle and doing the exactly same computation thing or even better in performance. \n",
    "\n",
    "The data in euron network is actually like the electricity-signal in our brain or in the CPU/GPU. \n",
    "\n",
    "Here the Author acchieve it with the most classic and most useful method ** Standardized **\n",
    "\n",
    "\n",
    "Second, to keep the differece is actually must be depending on the dependency.\n",
    "For example, the input data is actually keeping been operated by operations, layer by layer. \n",
    "The operation style is actually defining how we **Standardize** the data. \n",
    "\n",
    "If the operations are highly dependent, we just want to perform the data with same batch-normalize. \n",
    "\n",
    "On the other hand, if the operations itself is not highly dependent, then with each operation, we could do a batch-normalize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 核心思維\n",
    "\n",
    "物質因為差異所以存在。訊息的傳遞的編碼與解碼也是一樣的。只要訊號存在差異，基本上我們就可以分辨。\n",
    "\n",
    "因此大小的量度，其實意義不大。或者說，大小的定義，也是我們藉由一些測量或是交互或是操作，所得到具有差異的信息。\n",
    "\n",
    "因此只要該操作群是相互獨立的，或是獨立性高的，那重點是在於保持差異，那訊息基本上是無損的。\n",
    "\n",
    "但實際上，量度對物質有影響，因為量度也是透過其相依的測量/交互或操作所得到的。\n",
    "\n",
    "道可道，非常道。但我們可以舉一些例子：\n",
    "\n",
    "CPU的驅動電壓，從早期到現在基本上小了很多，但是這和處理的信息量其實是毫無相關的。\n",
    "\n",
    "現在的ＣＰＵ並不因位驅動電壓減小了，而使得處理的信息量變小。\n",
    "\n",
    "同理，我們腦中的神經元，處理的信息，其電壓更是微小。\n",
    "\n",
    "數據對於類神經網路，其實就相當於電壓或是電流變化相當於真正的神經元一樣。\n",
    "\n",
    "基本上，只要我們可以保持差異，那信息就不會丟失，只是對於物理量度來說，可能跨越不夠某些反應/交戶/操作，所需要的活化能，這其實是另一個量度世界的考量了。\n",
    "\n",
    "作者採用了標準化，保持相對差異。\n",
    "\n",
    "而保持差異因為存在，其實是依據(操作/交互/反應/量測)而定的。因為每一次(操作/交互/反應/量測)，就會產生新的量度。\n",
    "\n",
    "Pr( Difference | Operation )\n",
    "\n",
    "所以BN使用的方法時機，會根據操作不同 (conv 操作與 fully connect 操作)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expression in Math\n",
    "\n",
    "$ \\begin{array} \\\\\n",
    "\\text{Algorithm 1 : Batch Normalizing Transform, applied to activation x over a mini-batch. } \\\\  \n",
    "\\hline \n",
    "\\text{Input : Values of x over a mini-Batch : } B \\{  x_{1 \\text{ ... m }} \\} \\text{ ; Parameters to be learned : } \\beta \\text{ and } \\gamma \\\\\n",
    "\\text{Output : A set of } Y : \\{ y_{i} = \\text{ BatchNorm}_{\\beta, \\gamma}(x^{i}) \\} \\\\\n",
    "\\text{ } \\mu_{ \\beta } \\leftarrow  \\frac{1}{m} \\sum_{i=1}^m x_{i} \\text{ --- Mini-batch mean}\\\\\n",
    "\\text{ } \\mu_{ \\beta } \\leftarrow  \\frac{1}{m} \\sum_{i=1}^m (x_{i} - \\mu_{\\beta})^{2} \\text{ --- Mini-batch variance}\\\\\n",
    "\\end{array} $\n",
    "\n",
    "\n",
    "$ \\mu_{ \\beta } \\leftarrow  \\frac{1}{m} \\sum_{i=1}^m x_{i} \\text{ --- Mini-batch mean} $\n",
    "\n",
    "\n",
    "\n",
    "# Expression in Code \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning \n",
    "\n",
    "- https://github.com/ry/tensorflow-vgg16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "\n",
    "- https://github.com/pkmital/CADL/blob/master/session-4/libs/batch_norm.py\n",
    "\n",
    "- http://cthorey.github.io./backpropagation/\n",
    "\n",
    "- http://r2rt.com/implementing-batch-normalization-in-tensorflow.html\n",
    "\n",
    "- https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
    "\n",
    "- https://www.zhihu.com/question/38102762\n",
    "\n",
    "- http://shuokay.com/2016/10/15/wavenet/\n",
    "\n",
    "- http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html\n",
    "\n",
    "- http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from tensorflow.python import control_flow_ops\n",
    "\n",
    "def batch_norm(x, n_out, phase_train, scope='bn'):\n",
    "    \"\"\"\n",
    "    Batch normalization on convolutional maps.\n",
    "    Args:\n",
    "        x:           Tensor, 4D BHWD input maps\n",
    "        n_out:       integer, depth of input maps\n",
    "        phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "        scope:       string, variable scope\n",
    "    Return:\n",
    "        normed:      batch-normalized maps\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "                                     name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "                                      name='gamma', trainable=True)\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        mean, var = tf.cond(phase_train,\n",
    "                            mean_var_with_update,\n",
    "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /Users/kentchiu/deep_learning_research/data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /Users/kentchiu/deep_learning_research/data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /Users/kentchiu/deep_learning_research/data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /Users/kentchiu/deep_learning_research/data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mnist = input_data.read_data_sets('/Users/kentchiu/deep_learning_research/data/MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch Normalization Forward\n",
    "\n",
    "# Generate predetermined random weights so the networks are similarly initialized\n",
    "w1_initial = np.random.normal(size=(784,100)).astype(np.float32)\n",
    "w2_initial = np.random.normal(size=(100,100)).astype(np.float32)\n",
    "w3_initial = np.random.normal(size=(100,10)).astype(np.float32)\n",
    "\n",
    "# Small epsilon value for the BN transform\n",
    "epsilon = 1e-3\n",
    "\n",
    "# Placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 1 without BN\n",
    "w1 = tf.Variable(w1_initial)\n",
    "b1 = tf.Variable(tf.zeros([100]))\n",
    "z1 = tf.matmul(x,w1)+b1\n",
    "l1 = tf.nn.sigmoid(z1)\n",
    "\n",
    "# Layer 1 with BN\n",
    "w1_BN = tf.Variable(w1_initial)\n",
    "\n",
    "# Note that pre-batch normalization bias is ommitted. The effect of this bias would be\n",
    "# eliminated when subtracting the batch mean. Instead, the role of the bias is performed\n",
    "# by the new beta variable. See Section 3.2 of the BN2015 paper.\n",
    "z1_BN = tf.matmul(x,w1_BN)\n",
    "\n",
    "# Calculate batch mean and variance\n",
    "batch_mean1, batch_var1 = tf.nn.moments(z1_BN,[0])\n",
    "\n",
    "# Apply the initial batch normalizing transform\n",
    "z1_hat = (z1_BN - batch_mean1) / tf.sqrt(batch_var1 + epsilon)\n",
    "\n",
    "# Create two new parameters, scale and beta (shift)\n",
    "scale1 = tf.Variable(tf.ones([100]))\n",
    "beta1 = tf.Variable(tf.zeros([100]))\n",
    "\n",
    "# Scale and shift to obtain the final output of the batch normalization\n",
    "# this value is fed into the activation function (here a sigmoid)\n",
    "BN1 = scale1 * z1_hat + beta1\n",
    "l1_BN = tf.nn.sigmoid(BN1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Layer 2 without BN\n",
    "w2 = tf.Variable(w2_initial)\n",
    "b2 = tf.Variable(tf.zeros([100]))\n",
    "z2 = tf.matmul(l1,w2)+b2\n",
    "l2 = tf.nn.sigmoid(z2)\n",
    "\n",
    "# Layer 2 with BN, using Tensorflows built-in BN function\n",
    "w2_BN = tf.Variable(w2_initial)\n",
    "z2_BN = tf.matmul(l1_BN,w2_BN)\n",
    "batch_mean2, batch_var2 = tf.nn.moments(z2_BN,[0])\n",
    "scale2 = tf.Variable(tf.ones([100]))\n",
    "beta2 = tf.Variable(tf.zeros([100]))\n",
    "BN2 = tf.nn.batch_normalization(z2_BN,batch_mean2,batch_var2,beta2,scale2,epsilon)\n",
    "l2_BN = tf.nn.sigmoid(BN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Softmax\n",
    "w3 = tf.Variable(w3_initial)\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "y  = tf.nn.softmax(tf.matmul(l2,w3)+b3)\n",
    "\n",
    "w3_BN = tf.Variable(w3_initial)\n",
    "b3_BN = tf.Variable(tf.zeros([10]))\n",
    "y_BN  = tf.nn.softmax(tf.matmul(l2_BN,w3_BN)+b3_BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss, optimizer and predictions\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "cross_entropy_BN = -tf.reduce_sum(y_*tf.log(y_BN))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "train_step_BN = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy_BN)\n",
    "\n",
    "correct_prediction = tf.equal(tf.arg_max(y,1),tf.arg_max(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "correct_prediction_BN = tf.equal(tf.arg_max(y_BN,1),tf.arg_max(y_,1))\n",
    "accuracy_BN = tf.reduce_mean(tf.cast(correct_prediction_BN,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Train the Network \n",
    "\n",
    "zs, BNs, acc, acc_BN = [], [], [], []\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(40000):\n",
    "    batch = mnist.train.next_batch(60)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "    train_step_BN.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "    if i % 50 is 0:\n",
    "        res = sess.run([accuracy,accuracy_BN,z2,BN2],\n",
    "          feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "        acc.append(res[0])\n",
    "        acc_BN.append(res[1])\n",
    "        zs.append(np.mean(res[2],axis=0)) # record the mean value of z2 over the entire test set\n",
    "        BNs.append(np.mean(res[3],axis=0)) # record the mean value of BN2 over the entire test set\n",
    "        print (\".\")\n",
    "\n",
    "zs, BNs, acc, acc_BN = np.array(zs), np.array(BNs), np.array(acc), np.array(acc_BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEZCAYAAAC0HgObAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122a66510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(range(0,len(acc)*50,50),acc, label='Without BN')\n",
    "ax.plot(range(0,len(acc)*50,50),acc_BN, label='With BN')\n",
    "ax.set_xlabel('Training steps')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_ylim([0.8,1])\n",
    "ax.set_title('Batch Normalization Accuracy')\n",
    "ax.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Starting 1st session...\n",
      "Epoch: 0001 cost= 164.709009379\n",
      "Epoch: 0002 cost= 39.746273469\n",
      "Epoch: 0003 cost= 24.618006897\n",
      "First Optimization Finished!\n",
      "Accuracy: 0.9129\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Starting 2nd session...\n",
      "Model restored from file: /tmp/model.ckpt\n",
      "Epoch: 0001 cost= 17.003537263\n",
      "Epoch: 0002 cost= 12.180232949\n",
      "Epoch: 0003 cost= 9.019476439\n",
      "Epoch: 0004 cost= 6.707634071\n",
      "Epoch: 0005 cost= 5.014267356\n",
      "Epoch: 0006 cost= 3.689016492\n",
      "Epoch: 0007 cost= 2.808352564\n",
      "Second Optimization Finished!\n",
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Save and Restore a model using TensorFlow.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "model_path = \"/tmp/model.ckpt\"\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Running first session\n",
    "print(\"Starting 1st session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(3):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"First Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "# Running a new session\n",
    "print(\"Starting 2nd session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "    # Resume training\n",
    "    for epoch in range(7):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Second Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval(\n",
    "        {x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Output' object is not iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ab55f98f3978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_vars\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kent/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    507\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0minvoked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \"\"\"\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'Output' object is not iterable.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Output' object is not iterable."
     ]
    }
   ],
   "source": [
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 20, 20, 10], name='input')\n",
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "\n",
    "# generate random noise to pass into batch norm\n",
    "x_gen = tf.random_normal([50,20,20,10])\n",
    "pt_false = tf.Variable(tf.constant(True))\n",
    "\n",
    "#generate a constant variable to pass into batch norm\n",
    "y = x_gen.eval()\n",
    "\n",
    "[bn, bn_vars] = batch_norm(x, 10, phase_train)\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "train_step = lambda: bn.eval({x:x_gen.eval(), phase_train:True})\n",
    "test_step = lambda: bn.eval({x:y, phase_train:False})\n",
    "test_step_c = lambda: bn.eval({x:y, phase_train:True})\n",
    "\n",
    "# Verify that this is different as expected, two different x's have different norms\n",
    "print(train_step()[0][0][0])\n",
    "print(train_step()[0][0][0])\n",
    "\n",
    "# Verify that this is same as expected, same x's (y) have same norm\n",
    "print(train_step_c()[0][0][0])\n",
    "print(train_step_c()[0][0][0])\n",
    "\n",
    "# THIS IS DIFFERENT but should be they same, should only be reading from the ema.\n",
    "print(test_step()[0][0][0])\n",
    "print(test_step()[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-4d41582ae525>:16 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "n_in, n_out = 3, 16\n",
    "ksize = 3\n",
    "stride = 1\n",
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "input_image = tf.placeholder(tf.float32, name='input_image')\n",
    "kernel = tf.Variable(tf.truncated_normal([ksize, ksize, n_in, n_out],\n",
    "                                   stddev=math.sqrt(2.0/(ksize*ksize*n_out))),\n",
    "                                   name='kernel')\n",
    "conv = tf.nn.conv2d(input_image, kernel, [1,stride,stride,1], padding='SAME')\n",
    "conv_bn = batch_norm(conv, n_out, phase_train)\n",
    "relu = tf.nn.relu(conv_bn)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    for i in range(20):\n",
    "        test_image = np.random.rand(4,32,32,3)\n",
    "        sess_outputs = session.run([relu],\n",
    "          {input_image.name: test_image, phase_train.name: True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.engine import Layer, InputSpec\n",
    "from keras import initializations, regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class BatchNormalization(Layer):\n",
    "    '''Normalize the activations of the previous layer at each batch,\n",
    "    i.e. applies a transformation that maintains the mean activation\n",
    "    close to 0 and the activation standard deviation close to 1.\n",
    "    # Arguments\n",
    "        epsilon: small float > 0. Fuzz parameter.\n",
    "        mode: integer, 0, 1 or 2.\n",
    "            - 0: feature-wise normalization.\n",
    "                Each feature map in the input will\n",
    "                be normalized separately. The axis on which\n",
    "                to normalize is specified by the `axis` argument.\n",
    "                Note that if the input is a 4D image tensor\n",
    "                using Theano conventions (samples, channels, rows, cols)\n",
    "                then you should set `axis` to `1` to normalize along\n",
    "                the channels axis.\n",
    "                During training we use per-batch statistics to normalize\n",
    "                the data, and during testing we use running averages\n",
    "                computed during the training phase.\n",
    "            - 1: sample-wise normalization. This mode assumes a 2D input.\n",
    "            - 2: feature-wise normalization, like mode 0, but\n",
    "                using per-batch statistics to normalize the data during both\n",
    "                testing and training.\n",
    "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
    "            if your input tensor has shape (samples, channels, rows, cols),\n",
    "            set axis to 1 to normalize per feature map (channels axis).\n",
    "        momentum: momentum in the computation of the\n",
    "            exponential average of the mean and standard deviation\n",
    "            of the data, for feature-wise normalization.\n",
    "        weights: Initialization weights.\n",
    "            List of 2 Numpy arrays, with shapes:\n",
    "            `[(input_shape,), (input_shape,)]`\n",
    "            Note that the order of this list is [gamma, beta, mean, std]\n",
    "        beta_init: name of initialization function for shift parameter\n",
    "            (see [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_init: name of initialization function for scale parameter (see\n",
    "            [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the gamma vector.\n",
    "        beta_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the beta vector.\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    # References\n",
    "        - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://jmlr.org/proceedings/papers/v37/ioffe15.pdf)\n",
    "    '''\n",
    "    def __init__(self, epsilon=1e-5, mode=0, axis=-1, momentum=0.99,\n",
    "                 weights=None, beta_init='zero', gamma_init='one',\n",
    "                 gamma_regularizer=None, beta_regularizer=None, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.beta_init = initializations.get(beta_init)\n",
    "        self.gamma_init = initializations.get(gamma_init)\n",
    "        self.epsilon = epsilon\n",
    "        self.mode = mode\n",
    "        self.axis = axis\n",
    "        self.momentum = momentum\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.initial_weights = weights\n",
    "        if self.mode == 0:\n",
    "            self.uses_learning_phase = True\n",
    "        super(BatchNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (input_shape[self.axis],)\n",
    "\n",
    "        self.gamma = self.gamma_init(shape, name='{}_gamma'.format(self.name))\n",
    "        self.beta = self.beta_init(shape, name='{}_beta'.format(self.name))\n",
    "        self.trainable_weights = [self.gamma, self.beta]\n",
    "\n",
    "        self.regularizers = []\n",
    "        if self.gamma_regularizer:\n",
    "            self.gamma_regularizer.set_param(self.gamma)\n",
    "            self.regularizers.append(self.gamma_regularizer)\n",
    "\n",
    "        if self.beta_regularizer:\n",
    "            self.beta_regularizer.set_param(self.beta)\n",
    "            self.regularizers.append(self.beta_regularizer)\n",
    "\n",
    "        self.running_mean = K.zeros(shape,\n",
    "                                    name='{}_running_mean'.format(self.name))\n",
    "        self.running_std = K.ones(shape,\n",
    "                                  name='{}_running_std'.format(self.name))\n",
    "        self.non_trainable_weights = [self.running_mean, self.running_std]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "        self.called_with = None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if self.mode == 0 or self.mode == 2:\n",
    "            assert self.built, 'Layer must be built before being called'\n",
    "            input_shape = self.input_spec[0].shape\n",
    "\n",
    "            reduction_axes = list(range(len(input_shape)))\n",
    "            del reduction_axes[self.axis]\n",
    "            broadcast_shape = [1] * len(input_shape)\n",
    "            broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "            if self.mode == 2:\n",
    "                x_normed, mean, std = K.normalize_batch_in_training(\n",
    "                    x, self.gamma, self.beta, reduction_axes,\n",
    "                    epsilon=self.epsilon)\n",
    "            else:\n",
    "                # mode 0\n",
    "                if self.called_with not in {None, x}:\n",
    "                    raise Exception('You are attempting to share a '\n",
    "                                    'same `BatchNormalization` layer across '\n",
    "                                    'different data flows. '\n",
    "                                    'This is not possible. '\n",
    "                                    'You should use `mode=2` in '\n",
    "                                    '`BatchNormalization`, which has '\n",
    "                                    'a similar behavior but is shareable '\n",
    "                                    '(see docs for a description of '\n",
    "                                    'the behavior).')\n",
    "                self.called_with = x\n",
    "                x_normed, mean, std = K.normalize_batch_in_training(\n",
    "                    x, self.gamma, self.beta, reduction_axes,\n",
    "                    epsilon=self.epsilon)\n",
    "\n",
    "                self.updates = [K.moving_average_update(self.running_mean, mean, self.momentum),\n",
    "                                K.moving_average_update(self.running_std, std, self.momentum)]\n",
    "\n",
    "                if K.backend() == 'tensorflow' and sorted(reduction_axes) == range(K.ndim(x))[:-1]:\n",
    "                    x_normed_running = K.batch_normalization(\n",
    "                        x, self.running_mean, self.running_std,\n",
    "                        self.beta, self.gamma,\n",
    "                        epsilon=self.epsilon)\n",
    "                else:\n",
    "                    # need broadcasting\n",
    "                    broadcast_running_mean = K.reshape(self.running_mean, broadcast_shape)\n",
    "                    broadcast_running_std = K.reshape(self.running_std, broadcast_shape)\n",
    "                    broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "                    broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "                    x_normed_running = K.batch_normalization(\n",
    "                        x, broadcast_running_mean, broadcast_running_std,\n",
    "                        broadcast_beta, broadcast_gamma,\n",
    "                        epsilon=self.epsilon)\n",
    "\n",
    "                # pick the normalized form of x corresponding to the training phase\n",
    "                x_normed = K.in_train_phase(x_normed, x_normed_running)\n",
    "\n",
    "        elif self.mode == 1:\n",
    "            # sample-wise normalization\n",
    "            m = K.mean(x, axis=-1, keepdims=True)\n",
    "            std = K.sqrt(K.var(x, axis=-1, keepdims=True) + self.epsilon)\n",
    "            x_normed = (x - m) / (std + self.epsilon)\n",
    "            x_normed = self.gamma * x_normed + self.beta\n",
    "        return x_normed\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'epsilon': self.epsilon,\n",
    "                  'mode': self.mode,\n",
    "                  'axis': self.axis,\n",
    "                  'gamma_regularizer': self.gamma_regularizer.get_config() if self.gamma_regularizer else None,\n",
    "                  'beta_regularizer': self.beta_regularizer.get_config() if self.beta_regularizer else None,\n",
    "                  'momentum': self.momentum}\n",
    "        base_config = super(BatchNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.text_cell_render h1 {font-size: 2.4em;line-height:2.4em;text-align:left;}\n",
       "div.text_cell_render h3 {font-size: 1.8em;line-height:1.8em;text-align:left;}\n",
       "div.text_cell_render p {font-size: 1.4em;line-height:1.4em;text-align:left;}\n",
       "div.text_cell_render li {font-size: 1.0em;line-height:1.0em;text-align:left;}\n",
       "div.container pre{font-family: Monaco;font-size: 1.2em;line-height:1.2em;}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style>\n",
    "div.text_cell_render h1 {font-size: 2.4em;line-height:2.4em;text-align:left;}\n",
    "div.text_cell_render h3 {font-size: 1.8em;line-height:1.8em;text-align:left;}\n",
    "div.text_cell_render p {font-size: 1.4em;line-height:1.4em;text-align:left;}\n",
    "div.text_cell_render li {font-size: 1.0em;line-height:1.0em;text-align:left;}\n",
    "div.container pre{font-family: Monaco;font-size: 1.2em;line-height:1.2em;}\n",
    "</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- [CS231 Youtube](https://www.youtube.com/watch?v=Vf_-OkqbwPo&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&index=12)\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30, 30, 3)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder_14' with dtype float\n\t [[Node: Placeholder_14 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'Placeholder_14', defined at:\n  File \"/Users/kentchiu/anaconda/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-34-e38abf459f07>\", line 6, in <module>\n    tf_x = tf.placeholder(\"float\", [None, 30,30,3])\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1332, in placeholder\n    name=name)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1748, in _placeholder\n    name=name)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_14' with dtype float\n\t [[Node: Placeholder_14 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e38abf459f07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \"\"\"\n\u001b[0;32m--> 559\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3759\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3761\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder_14' with dtype float\n\t [[Node: Placeholder_14 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'Placeholder_14', defined at:\n  File \"/Users/kentchiu/anaconda/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-34-e38abf459f07>\", line 6, in <module>\n    tf_x = tf.placeholder(\"float\", [None, 30,30,3])\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1332, in placeholder\n    name=name)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1748, in _placeholder\n    name=name)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\n    op_def=op_def)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/kentchiu/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder_14' with dtype float\n\t [[Node: Placeholder_14 = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.random(size=(10,30,30,3))\n",
    "print (x.shape)\n",
    "\n",
    "tf_x = tf.placeholder(\"float\", [None, 30,30,3])\n",
    "\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    session.run(tf.initialize_all_variables())\n",
    "    print (tf_x.eval() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
