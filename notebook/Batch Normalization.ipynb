{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167v3.pdf)\n",
    "\n",
    "$$ \\text{By Kent Chiu @ 20161115} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.engine import Layer, InputSpec\n",
    "from keras import initializations, regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class BatchNormalization(Layer):\n",
    "    '''Normalize the activations of the previous layer at each batch,\n",
    "    i.e. applies a transformation that maintains the mean activation\n",
    "    close to 0 and the activation standard deviation close to 1.\n",
    "    # Arguments\n",
    "        epsilon: small float > 0. Fuzz parameter.\n",
    "        mode: integer, 0, 1 or 2.\n",
    "            - 0: feature-wise normalization.\n",
    "                Each feature map in the input will\n",
    "                be normalized separately. The axis on which\n",
    "                to normalize is specified by the `axis` argument.\n",
    "                Note that if the input is a 4D image tensor\n",
    "                using Theano conventions (samples, channels, rows, cols)\n",
    "                then you should set `axis` to `1` to normalize along\n",
    "                the channels axis.\n",
    "                During training we use per-batch statistics to normalize\n",
    "                the data, and during testing we use running averages\n",
    "                computed during the training phase.\n",
    "            - 1: sample-wise normalization. This mode assumes a 2D input.\n",
    "            - 2: feature-wise normalization, like mode 0, but\n",
    "                using per-batch statistics to normalize the data during both\n",
    "                testing and training.\n",
    "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
    "            if your input tensor has shape (samples, channels, rows, cols),\n",
    "            set axis to 1 to normalize per feature map (channels axis).\n",
    "        momentum: momentum in the computation of the\n",
    "            exponential average of the mean and standard deviation\n",
    "            of the data, for feature-wise normalization.\n",
    "        weights: Initialization weights.\n",
    "            List of 2 Numpy arrays, with shapes:\n",
    "            `[(input_shape,), (input_shape,)]`\n",
    "            Note that the order of this list is [gamma, beta, mean, std]\n",
    "        beta_init: name of initialization function for shift parameter\n",
    "            (see [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_init: name of initialization function for scale parameter (see\n",
    "            [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the gamma vector.\n",
    "        beta_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the beta vector.\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    # References\n",
    "        - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://jmlr.org/proceedings/papers/v37/ioffe15.pdf)\n",
    "    '''\n",
    "    def __init__(self, epsilon=1e-5, mode=0, axis=-1, momentum=0.99,\n",
    "                 weights=None, beta_init='zero', gamma_init='one',\n",
    "                 gamma_regularizer=None, beta_regularizer=None, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.beta_init = initializations.get(beta_init)\n",
    "        self.gamma_init = initializations.get(gamma_init)\n",
    "        self.epsilon = epsilon\n",
    "        self.mode = mode\n",
    "        self.axis = axis\n",
    "        self.momentum = momentum\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.initial_weights = weights\n",
    "        if self.mode == 0:\n",
    "            self.uses_learning_phase = True\n",
    "        super(BatchNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (input_shape[self.axis],)\n",
    "\n",
    "        self.gamma = self.gamma_init(shape, name='{}_gamma'.format(self.name))\n",
    "        self.beta = self.beta_init(shape, name='{}_beta'.format(self.name))\n",
    "        self.trainable_weights = [self.gamma, self.beta]\n",
    "\n",
    "        self.regularizers = []\n",
    "        if self.gamma_regularizer:\n",
    "            self.gamma_regularizer.set_param(self.gamma)\n",
    "            self.regularizers.append(self.gamma_regularizer)\n",
    "\n",
    "        if self.beta_regularizer:\n",
    "            self.beta_regularizer.set_param(self.beta)\n",
    "            self.regularizers.append(self.beta_regularizer)\n",
    "\n",
    "        self.running_mean = K.zeros(shape,\n",
    "                                    name='{}_running_mean'.format(self.name))\n",
    "        self.running_std = K.ones(shape,\n",
    "                                  name='{}_running_std'.format(self.name))\n",
    "        self.non_trainable_weights = [self.running_mean, self.running_std]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "        self.called_with = None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if self.mode == 0 or self.mode == 2:\n",
    "            assert self.built, 'Layer must be built before being called'\n",
    "            input_shape = self.input_spec[0].shape\n",
    "\n",
    "            reduction_axes = list(range(len(input_shape)))\n",
    "            del reduction_axes[self.axis]\n",
    "            broadcast_shape = [1] * len(input_shape)\n",
    "            broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "            if self.mode == 2:\n",
    "                x_normed, mean, std = K.normalize_batch_in_training(\n",
    "                    x, self.gamma, self.beta, reduction_axes,\n",
    "                    epsilon=self.epsilon)\n",
    "            else:\n",
    "                # mode 0\n",
    "                if self.called_with not in {None, x}:\n",
    "                    raise Exception('You are attempting to share a '\n",
    "                                    'same `BatchNormalization` layer across '\n",
    "                                    'different data flows. '\n",
    "                                    'This is not possible. '\n",
    "                                    'You should use `mode=2` in '\n",
    "                                    '`BatchNormalization`, which has '\n",
    "                                    'a similar behavior but is shareable '\n",
    "                                    '(see docs for a description of '\n",
    "                                    'the behavior).')\n",
    "                self.called_with = x\n",
    "                x_normed, mean, std = K.normalize_batch_in_training(\n",
    "                    x, self.gamma, self.beta, reduction_axes,\n",
    "                    epsilon=self.epsilon)\n",
    "\n",
    "                self.updates = [K.moving_average_update(self.running_mean, mean, self.momentum),\n",
    "                                K.moving_average_update(self.running_std, std, self.momentum)]\n",
    "\n",
    "                if K.backend() == 'tensorflow' and sorted(reduction_axes) == range(K.ndim(x))[:-1]:\n",
    "                    x_normed_running = K.batch_normalization(\n",
    "                        x, self.running_mean, self.running_std,\n",
    "                        self.beta, self.gamma,\n",
    "                        epsilon=self.epsilon)\n",
    "                else:\n",
    "                    # need broadcasting\n",
    "                    broadcast_running_mean = K.reshape(self.running_mean, broadcast_shape)\n",
    "                    broadcast_running_std = K.reshape(self.running_std, broadcast_shape)\n",
    "                    broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "                    broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "                    x_normed_running = K.batch_normalization(\n",
    "                        x, broadcast_running_mean, broadcast_running_std,\n",
    "                        broadcast_beta, broadcast_gamma,\n",
    "                        epsilon=self.epsilon)\n",
    "\n",
    "                # pick the normalized form of x corresponding to the training phase\n",
    "                x_normed = K.in_train_phase(x_normed, x_normed_running)\n",
    "\n",
    "        elif self.mode == 1:\n",
    "            # sample-wise normalization\n",
    "            m = K.mean(x, axis=-1, keepdims=True)\n",
    "            std = K.sqrt(K.var(x, axis=-1, keepdims=True) + self.epsilon)\n",
    "            x_normed = (x - m) / (std + self.epsilon)\n",
    "            x_normed = self.gamma * x_normed + self.beta\n",
    "        return x_normed\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'epsilon': self.epsilon,\n",
    "                  'mode': self.mode,\n",
    "                  'axis': self.axis,\n",
    "                  'gamma_regularizer': self.gamma_regularizer.get_config() if self.gamma_regularizer else None,\n",
    "                  'beta_regularizer': self.beta_regularizer.get_config() if self.beta_regularizer else None,\n",
    "                  'momentum': self.momentum}\n",
    "        base_config = super(BatchNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.text_cell_render h1 {font-size: 2.4em;line-height:2.4em;text-align:left;}\n",
       "div.text_cell_render h3 {font-size: 1.8em;line-height:1.8em;text-align:left;}\n",
       "div.text_cell_render p {font-size: 1.4em;line-height:1.4em;text-align:left;}\n",
       "div.text_cell_render li {font-size: 1.0em;line-height:1.0em;text-align:left;}\n",
       "div.container pre{font-family: Monaco;font-size: 1.2em;line-height:1.2em;}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style>\n",
    "div.text_cell_render h1 {font-size: 2.4em;line-height:2.4em;text-align:left;}\n",
    "div.text_cell_render h3 {font-size: 1.8em;line-height:1.8em;text-align:left;}\n",
    "div.text_cell_render p {font-size: 1.4em;line-height:1.4em;text-align:left;}\n",
    "div.text_cell_render li {font-size: 1.0em;line-height:1.0em;text-align:left;}\n",
    "div.container pre{font-family: Monaco;font-size: 1.2em;line-height:1.2em;}\n",
    "</style>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
