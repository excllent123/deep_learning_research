{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167v3.pdf)\n",
    "\n",
    "$$ \\text{By Kent Chiu @ 20161115} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Concept \n",
    "\n",
    "For me, the core concept is the **mapping ** & **difference.** \n",
    "\n",
    "First : Keep Difference is the thing that matters. \n",
    "Since the neuron network is just acting like the real neuron, the relative difference is the true thing that really matters. \n",
    "\n",
    "So basically, as long as we keep its relative difference, the neuron network could still works well. \n",
    "\n",
    "However, the actual amplitude, we could actually see it as the energy or voltage or whatever you like, just CPU nowadays could be drive in a lower votagle and doing the exactly same computation thing or even better in performance. \n",
    "\n",
    "The data in euron network is actually like the electricity-signal in our brain or in the CPU/GPU. \n",
    "\n",
    "Here the Author acchieve it with the most classic and most useful method ** Standardized **\n",
    "\n",
    "\n",
    "Second, to keep the differece is actually must be depending on the dependency.\n",
    "For example, the input data is actually keeping been operated by operations, layer by layer. \n",
    "The operation style is actually defining how we **Standardize** the data. \n",
    "\n",
    "If the operations are highly dependent, we just want to perform the data with same batch-normalize. \n",
    "\n",
    "On the other hand, if the operations itself is not highly dependent, then with each operation, we could do a batch-normalize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 核心思維\n",
    "\n",
    "物質因為差異所以存在。訊息的傳遞的編碼與解碼也是一樣的。只要訊號存在差異，基本上我們就可以分辨。\n",
    "\n",
    "因此大小的量度，其實意義不大。或者說，大小的定義，也是我們藉由一些測量或是交互或是操作，所得到具有差異的信息。\n",
    "\n",
    "因此只要該操作群是相互獨立的，或是獨立性高的，那重點是在於保持差異，那訊息基本上是無損的。\n",
    "\n",
    "但實際上，量度對物質有影響，因為量度也是透過其相依的測量/交互或操作所得到的。\n",
    "\n",
    "道可道，非常道。但我們可以舉一些例子：\n",
    "\n",
    "CPU的驅動電壓，從早期到現在基本上小了很多，但是這和處理的信息量其實是毫無相關的。\n",
    "\n",
    "現在的ＣＰＵ並不因位驅動電壓減小了，而使得處理的信息量變小。\n",
    "\n",
    "同理，我們腦中的神經元，處理的信息，其電壓更是微小。\n",
    "\n",
    "數據對於類神經網路，其實就相當於電壓或是電流變化相當於真正的神經元一樣。\n",
    "\n",
    "基本上，只要我們可以保持差異，那信息就不會丟失，只是對於物理量度來說，可能跨越不夠某些反應/交戶/操作，所需要的活化能，這其實是另一個量度世界的考量了。\n",
    "\n",
    "作者採用了標準化，保持相對差異。\n",
    "\n",
    "而保持差異因為存在，其實是依據(操作/交互/反應/量測)而定的。因為每一次(操作/交互/反應/量測)，就會產生新的量度。\n",
    "\n",
    "Pr( Difference | Operation )\n",
    "\n",
    "所以BN使用的方法時機，會根據操作不同 (conv 操作與 fully connect 操作)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expression in Math\n",
    "\n",
    "$ \\begin{array} \\\\\n",
    "\\text{Algorithm 1 : Batch Normalizing Transform, applied to activation x over a mini-batch. } \\\\  \n",
    "\\hline \n",
    "\\text{Input : Values of x over a mini-Batch : } B \\{  x_{1 \\text{ ... m }} \\} \\text{ ; Parameters to be learned : } \\beta \\text{ and } \\gamma \\\\\n",
    "\\text{Output : A set of } Y : \\{ y_{i} = \\text{ BatchNorm}_{\\beta, \\gamma}(x^{i}) \\} \\\\\n",
    "\\text{ } \\mu_{ \\beta } \\leftarrow  \\frac{1}{m} \\sum_{i=1}^m x_{i} \\text{ --- Mini-batch mean}\\\\\n",
    "\\text{ } \\mu_{ \\beta } \\leftarrow  \\frac{1}{m} \\sum_{i=1}^m (x_{i} - \\mu_{\\beta})^{2} \\text{ --- Mini-batch variance}\\\\\n",
    "\\end{array} $\n",
    "\n",
    "\n",
    "$ \\mu_{ \\beta } \\leftarrow  \\frac{1}{m} \\sum_{i=1}^m x_{i} \\text{ --- Mini-batch mean} $\n",
    "\n",
    "\n",
    "\n",
    "# Expression in Code \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning \n",
    "\n",
    "- https://github.com/ry/tensorflow-vgg16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "\n",
    "- https://github.com/pkmital/CADL/blob/master/session-4/libs/batch_norm.py\n",
    "\n",
    "- http://cthorey.github.io./backpropagation/\n",
    "\n",
    "- http://r2rt.com/implementing-batch-normalization-in-tensorflow.html\n",
    "\n",
    "- https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
    "\n",
    "- https://www.zhihu.com/question/38102762\n",
    "\n",
    "- http://shuokay.com/2016/10/15/wavenet/\n",
    "\n",
    "- http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html\n",
    "\n",
    "- http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from tensorflow.python import control_flow_ops\n",
    "\n",
    "def batch_norm(x, n_out, phase_train, scope='bn'):\n",
    "    \"\"\"\n",
    "    Batch normalization on convolutional maps.\n",
    "    Args:\n",
    "        x:           Tensor, 4D BHWD input maps\n",
    "        n_out:       integer, depth of input maps\n",
    "        phase_train: boolean tf.Varialbe, true indicates training phase\n",
    "        scope:       string, variable scope\n",
    "    Return:\n",
    "        normed:      batch-normalized maps\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "                                     name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "                                      name='gamma', trainable=True)\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "\n",
    "        mean, var = tf.cond(phase_train,\n",
    "                            mean_var_with_update,\n",
    "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "        normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)\n",
    "    return normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /Users/kentchiu/deep_learning_research/data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /Users/kentchiu/deep_learning_research/data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /Users/kentchiu/deep_learning_research/data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /Users/kentchiu/deep_learning_research/data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mnist = input_data.read_data_sets('/Users/kentchiu/deep_learning_research/data/MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch Normalization Forward\n",
    "\n",
    "# Generate predetermined random weights so the networks are similarly initialized\n",
    "w1_initial = np.random.normal(size=(784,100)).astype(np.float32)\n",
    "w2_initial = np.random.normal(size=(100,100)).astype(np.float32)\n",
    "w3_initial = np.random.normal(size=(100,10)).astype(np.float32)\n",
    "\n",
    "# Small epsilon value for the BN transform\n",
    "epsilon = 1e-3\n",
    "\n",
    "# Placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 1 without BN\n",
    "w1 = tf.Variable(w1_initial)\n",
    "b1 = tf.Variable(tf.zeros([100]))\n",
    "z1 = tf.matmul(x,w1)+b1\n",
    "l1 = tf.nn.sigmoid(z1)\n",
    "\n",
    "# Layer 1 with BN\n",
    "w1_BN = tf.Variable(w1_initial)\n",
    "\n",
    "# Note that pre-batch normalization bias is ommitted. The effect of this bias would be\n",
    "# eliminated when subtracting the batch mean. Instead, the role of the bias is performed\n",
    "# by the new beta variable. See Section 3.2 of the BN2015 paper.\n",
    "z1_BN = tf.matmul(x,w1_BN)\n",
    "\n",
    "# Calculate batch mean and variance\n",
    "batch_mean1, batch_var1 = tf.nn.moments(z1_BN,[0])\n",
    "\n",
    "# Apply the initial batch normalizing transform\n",
    "z1_hat = (z1_BN - batch_mean1) / tf.sqrt(batch_var1 + epsilon)\n",
    "\n",
    "# Create two new parameters, scale and beta (shift)\n",
    "scale1 = tf.Variable(tf.ones([100]))\n",
    "beta1 = tf.Variable(tf.zeros([100]))\n",
    "\n",
    "# Scale and shift to obtain the final output of the batch normalization\n",
    "# this value is fed into the activation function (here a sigmoid)\n",
    "BN1 = scale1 * z1_hat + beta1\n",
    "l1_BN = tf.nn.sigmoid(BN1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Layer 2 without BN\n",
    "w2 = tf.Variable(w2_initial)\n",
    "b2 = tf.Variable(tf.zeros([100]))\n",
    "z2 = tf.matmul(l1,w2)+b2\n",
    "l2 = tf.nn.sigmoid(z2)\n",
    "\n",
    "# Layer 2 with BN, using Tensorflows built-in BN function\n",
    "w2_BN = tf.Variable(w2_initial)\n",
    "z2_BN = tf.matmul(l1_BN,w2_BN)\n",
    "batch_mean2, batch_var2 = tf.nn.moments(z2_BN,[0])\n",
    "scale2 = tf.Variable(tf.ones([100]))\n",
    "beta2 = tf.Variable(tf.zeros([100]))\n",
    "BN2 = tf.nn.batch_normalization(z2_BN,batch_mean2,batch_var2,beta2,scale2,epsilon)\n",
    "l2_BN = tf.nn.sigmoid(BN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Softmax\n",
    "w3 = tf.Variable(w3_initial)\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "y  = tf.nn.softmax(tf.matmul(l2,w3)+b3)\n",
    "\n",
    "w3_BN = tf.Variable(w3_initial)\n",
    "b3_BN = tf.Variable(tf.zeros([10]))\n",
    "y_BN  = tf.nn.softmax(tf.matmul(l2_BN,w3_BN)+b3_BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss, optimizer and predictions\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "cross_entropy_BN = -tf.reduce_sum(y_*tf.log(y_BN))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "train_step_BN = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy_BN)\n",
    "\n",
    "correct_prediction = tf.equal(tf.arg_max(y,1),tf.arg_max(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "correct_prediction_BN = tf.equal(tf.arg_max(y_BN,1),tf.arg_max(y_,1))\n",
    "accuracy_BN = tf.reduce_mean(tf.cast(correct_prediction_BN,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Train the Network \n",
    "\n",
    "zs, BNs, acc, acc_BN = [], [], [], []\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(40000):\n",
    "    batch = mnist.train.next_batch(60)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "    train_step_BN.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "    if i % 50 is 0:\n",
    "        res = sess.run([accuracy,accuracy_BN,z2,BN2],\n",
    "          feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "        acc.append(res[0])\n",
    "        acc_BN.append(res[1])\n",
    "        zs.append(np.mean(res[2],axis=0)) # record the mean value of z2 over the entire test set\n",
    "        BNs.append(np.mean(res[3],axis=0)) # record the mean value of BN2 over the entire test set\n",
    "        print (\".\")\n",
    "\n",
    "zs, BNs, acc, acc_BN = np.array(zs), np.array(BNs), np.array(acc), np.array(acc_BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEZCAYAAAC0HgObAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122a66510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(range(0,len(acc)*50,50),acc, label='Without BN')\n",
    "ax.plot(range(0,len(acc)*50,50),acc_BN, label='With BN')\n",
    "ax.set_xlabel('Training steps')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_ylim([0.8,1])\n",
    "ax.set_title('Batch Normalization Accuracy')\n",
    "ax.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Starting 1st session...\n",
      "Epoch: 0001 cost= 164.709009379\n",
      "Epoch: 0002 cost= 39.746273469\n",
      "Epoch: 0003 cost= 24.618006897\n",
      "First Optimization Finished!\n",
      "Accuracy: 0.9129\n",
      "Model saved in file: /tmp/model.ckpt\n",
      "Starting 2nd session...\n",
      "Model restored from file: /tmp/model.ckpt\n",
      "Epoch: 0001 cost= 17.003537263\n",
      "Epoch: 0002 cost= 12.180232949\n",
      "Epoch: 0003 cost= 9.019476439\n",
      "Epoch: 0004 cost= 6.707634071\n",
      "Epoch: 0005 cost= 5.014267356\n",
      "Epoch: 0006 cost= 3.689016492\n",
      "Epoch: 0007 cost= 2.808352564\n",
      "Second Optimization Finished!\n",
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Save and Restore a model using TensorFlow.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "model_path = \"/tmp/model.ckpt\"\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# 'Saver' op to save and restore all the variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Running first session\n",
    "print(\"Starting 1st session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(3):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"First Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "    # Save model weights to disk\n",
    "    save_path = saver.save(sess, model_path)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "# Running a new session\n",
    "print(\"Starting 2nd session...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, model_path)\n",
    "    print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "    # Resume training\n",
    "    for epoch in range(7):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Second Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval(\n",
    "        {x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Output' object is not iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ab55f98f3978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_vars\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kent/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    507\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0minvoked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \"\"\"\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'Output' object is not iterable.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Output' object is not iterable."
     ]
    }
   ],
   "source": [
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 20, 20, 10], name='input')\n",
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "\n",
    "# generate random noise to pass into batch norm\n",
    "x_gen = tf.random_normal([50,20,20,10])\n",
    "pt_false = tf.Variable(tf.constant(True))\n",
    "\n",
    "#generate a constant variable to pass into batch norm\n",
    "y = x_gen.eval()\n",
    "\n",
    "[bn, bn_vars] = batch_norm(x, 10, phase_train)\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "train_step = lambda: bn.eval({x:x_gen.eval(), phase_train:True})\n",
    "test_step = lambda: bn.eval({x:y, phase_train:False})\n",
    "test_step_c = lambda: bn.eval({x:y, phase_train:True})\n",
    "\n",
    "# Verify that this is different as expected, two different x's have different norms\n",
    "print(train_step()[0][0][0])\n",
    "print(train_step()[0][0][0])\n",
    "\n",
    "# Verify that this is same as expected, same x's (y) have same norm\n",
    "print(train_step_c()[0][0][0])\n",
    "print(train_step_c()[0][0][0])\n",
    "\n",
    "# THIS IS DIFFERENT but should be they same, should only be reading from the ema.\n",
    "print(test_step()[0][0][0])\n",
    "print(test_step()[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-4d41582ae525>:16 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "n_in, n_out = 3, 16\n",
    "ksize = 3\n",
    "stride = 1\n",
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "input_image = tf.placeholder(tf.float32, name='input_image')\n",
    "kernel = tf.Variable(tf.truncated_normal([ksize, ksize, n_in, n_out],\n",
    "                                   stddev=math.sqrt(2.0/(ksize*ksize*n_out))),\n",
    "                                   name='kernel')\n",
    "conv = tf.nn.conv2d(input_image, kernel, [1,stride,stride,1], padding='SAME')\n",
    "conv_bn = batch_norm(conv, n_out, phase_train)\n",
    "relu = tf.nn.relu(conv_bn)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    for i in range(20):\n",
    "        test_image = np.random.rand(4,32,32,3)\n",
    "        sess_outputs = session.run([relu],\n",
    "          {input_image.name: test_image, phase_train.name: True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.engine import Layer, InputSpec\n",
    "from keras import initializations, regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class BatchNormalization(Layer):\n",
    "    '''Normalize the activations of the previous layer at each batch,\n",
    "    i.e. applies a transformation that maintains the mean activation\n",
    "    close to 0 and the activation standard deviation close to 1.\n",
    "    # Arguments\n",
    "        epsilon: small float > 0. Fuzz parameter.\n",
    "        mode: integer, 0, 1 or 2.\n",
    "            - 0: feature-wise normalization.\n",
    "                Each feature map in the input will\n",
    "                be normalized separately. The axis on which\n",
    "                to normalize is specified by the `axis` argument.\n",
    "                Note that if the input is a 4D image tensor\n",
    "                using Theano conventions (samples, channels, rows, cols)\n",
    "                then you should set `axis` to `1` to normalize along\n",
    "                the channels axis.\n",
    "                During training we use per-batch statistics to normalize\n",
    "                the data, and during testing we use running averages\n",
    "                computed during the training phase.\n",
    "            - 1: sample-wise normalization. This mode assumes a 2D input.\n",
    "            - 2: feature-wise normalization, like mode 0, but\n",
    "                using per-batch statistics to normalize the data during both\n",
    "                testing and training.\n",
    "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
    "            if your input tensor has shape (samples, channels, rows, cols),\n",
    "            set axis to 1 to normalize per feature map (channels axis).\n",
    "        momentum: momentum in the computation of the\n",
    "            exponential average of the mean and standard deviation\n",
    "            of the data, for feature-wise normalization.\n",
    "        weights: Initialization weights.\n",
    "            List of 2 Numpy arrays, with shapes:\n",
    "            `[(input_shape,), (input_shape,)]`\n",
    "            Note that the order of this list is [gamma, beta, mean, std]\n",
    "        beta_init: name of initialization function for shift parameter\n",
    "            (see [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_init: name of initialization function for scale parameter (see\n",
    "            [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the gamma vector.\n",
    "        beta_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the beta vector.\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    # References\n",
    "        - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://jmlr.org/proceedings/papers/v37/ioffe15.pdf)\n",
    "    '''\n",
    "    def __init__(self, epsilon=1e-5, mode=0, axis=-1, momentum=0.99,\n",
    "                 weights=None, beta_init='zero', gamma_init='one',\n",
    "                 gamma_regularizer=None, beta_regularizer=None, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.beta_init = initializations.get(beta_init)\n",
    "        self.gamma_init = initializations.get(gamma_init)\n",
    "        self.epsilon = epsilon\n",
    "        self.mode = mode\n",
    "        self.axis = axis\n",
    "        self.momentum = momentum\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.initial_weights = weights\n",
    "        if self.mode == 0:\n",
    "            self.uses_learning_phase = True\n",
    "        super(BatchNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (input_shape[self.axis],)\n",
    "\n",
    "        self.gamma = self.gamma_init(shape, name='{}_gamma'.format(self.name))\n",
    "        self.beta = self.beta_init(shape, name='{}_beta'.format(self.name))\n",
    "        self.trainable_weights = [self.gamma, self.beta]\n",
    "\n",
    "        self.regularizers = []\n",
    "        if self.gamma_regularizer:\n",
    "            self.gamma_regularizer.set_param(self.gamma)\n",
    "            self.regularizers.append(self.gamma_regularizer)\n",
    "\n",
    "        if self.beta_regularizer:\n",
    "            self.beta_regularizer.set_param(self.beta)\n",
    "            self.regularizers.append(self.beta_regularizer)\n",
    "\n",
    "        self.running_mean = K.zeros(shape,\n",
    "                                    name='{}_running_mean'.format(self.name))\n",
    "        self.running_std = K.ones(shape,\n",
    "                                  name='{}_running_std'.format(self.name))\n",
    "        self.non_trainable_weights = [self.running_mean, self.running_std]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "        self.called_with = None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if self.mode == 0 or self.mode == 2:\n",
    "            assert self.built, 'Layer must be built before being called'\n",
    "            input_shape = self.input_spec[0].shape\n",
    "\n",
    "            reduction_axes = list(range(len(input_shape)))\n",
    "            del reduction_axes[self.axis]\n",
    "            broadcast_shape = [1] * len(input_shape)\n",
    "            broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "            if self.mode == 2:\n",
    "                x_normed, mean, std = K.normalize_batch_in_training(\n",
    "                    x, self.gamma, self.beta, reduction_axes,\n",
    "                    epsilon=self.epsilon)\n",
    "            else:\n",
    "                # mode 0\n",
    "                if self.called_with not in {None, x}:\n",
    "                    raise Exception('You are attempting to share a '\n",
    "                                    'same `BatchNormalization` layer across '\n",
    "                                    'different data flows. '\n",
    "                                    'This is not possible. '\n",
    "                                    'You should use `mode=2` in '\n",
    "                                    '`BatchNormalization`, which has '\n",
    "                                    'a similar behavior but is shareable '\n",
    "                                    '(see docs for a description of '\n",
    "                                    'the behavior).')\n",
    "                self.called_with = x\n",
    "                x_normed, mean, std = K.normalize_batch_in_training(\n",
    "                    x, self.gamma, self.beta, reduction_axes,\n",
    "                    epsilon=self.epsilon)\n",
    "\n",
    "                self.updates = [K.moving_average_update(self.running_mean, mean, self.momentum),\n",
    "                                K.moving_average_update(self.running_std, std, self.momentum)]\n",
    "\n",
    "                if K.backend() == 'tensorflow' and sorted(reduction_axes) == range(K.ndim(x))[:-1]:\n",
    "                    x_normed_running = K.batch_normalization(\n",
    "                        x, self.running_mean, self.running_std,\n",
    "                        self.beta, self.gamma,\n",
    "                        epsilon=self.epsilon)\n",
    "                else:\n",
    "                    # need broadcasting\n",
    "                    broadcast_running_mean = K.reshape(self.running_mean, broadcast_shape)\n",
    "                    broadcast_running_std = K.reshape(self.running_std, broadcast_shape)\n",
    "                    broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "                    broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "                    x_normed_running = K.batch_normalization(\n",
    "                        x, broadcast_running_mean, broadcast_running_std,\n",
    "                        broadcast_beta, broadcast_gamma,\n",
    "                        epsilon=self.epsilon)\n",
    "\n",
    "                # pick the normalized form of x corresponding to the training phase\n",
    "                x_normed = K.in_train_phase(x_normed, x_normed_running)\n",
    "\n",
    "        elif self.mode == 1:\n",
    "            # sample-wise normalization\n",
    "            m = K.mean(x, axis=-1, keepdims=True)\n",
    "            std = K.sqrt(K.var(x, axis=-1, keepdims=True) + self.epsilon)\n",
    "            x_normed = (x - m) / (std + self.epsilon)\n",
    "            x_normed = self.gamma * x_normed + self.beta\n",
    "        return x_normed\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'epsilon': self.epsilon,\n",
    "                  'mode': self.mode,\n",
    "                  'axis': self.axis,\n",
    "                  'gamma_regularizer': self.gamma_regularizer.get_config() if self.gamma_regularizer else None,\n",
    "                  'beta_regularizer': self.beta_regularizer.get_config() if self.beta_regularizer else None,\n",
    "                  'momentum': self.momentum}\n",
    "        base_config = super(BatchNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.text_cell_render h1 {font-size: 2.4em;line-height:2.4em;text-align:left;}\n",
       "div.text_cell_render h3 {font-size: 1.8em;line-height:1.8em;text-align:left;}\n",
       "div.text_cell_render p {font-size: 1.4em;line-height:1.4em;text-align:left;}\n",
       "div.text_cell_render li {font-size: 1.0em;line-height:1.0em;text-align:left;}\n",
       "div.container pre{font-family: Monaco;font-size: 1.2em;line-height:1.2em;}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style>\n",
    "div.text_cell_render h1 {font-size: 2.4em;line-height:2.4em;text-align:left;}\n",
    "div.text_cell_render h3 {font-size: 1.8em;line-height:1.8em;text-align:left;}\n",
    "div.text_cell_render p {font-size: 1.4em;line-height:1.4em;text-align:left;}\n",
    "div.text_cell_render li {font-size: 1.0em;line-height:1.0em;text-align:left;}\n",
    "div.container pre{font-family: Monaco;font-size: 1.2em;line-height:1.2em;}\n",
    "</style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- [CS231 Youtube](https://www.youtube.com/watch?v=Vf_-OkqbwPo&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&index=12)\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30, 30, 3)\n",
      "[[[[ 0.89832491  0.74073259  0.69702154]\n",
      "   [ 0.41813199  0.30042932  0.55091663]\n",
      "   [ 0.17186376  0.71949197  0.75365476]\n",
      "   ..., \n",
      "   [ 0.81863702  0.56082664  0.7038035 ]\n",
      "   [ 0.40530724  0.94762405  0.66999269]\n",
      "   [ 0.12545994  0.70678033  0.86147227]]\n",
      "\n",
      "  [[ 0.47938645  0.66544564  0.02400668]\n",
      "   [ 0.00351703  0.43009897  0.35642966]\n",
      "   [ 0.87072869  0.64561302  0.33936732]\n",
      "   ..., \n",
      "   [ 0.13518703  0.75539665  0.66291676]\n",
      "   [ 0.24880344  0.7110323   0.20449154]\n",
      "   [ 0.01424011  0.55126724  0.62524347]]\n",
      "\n",
      "  [[ 0.24424882  0.45313095  0.58476978]\n",
      "   [ 0.86441355  0.46237933  0.85738481]\n",
      "   [ 0.70850982  0.87347843  0.68618125]\n",
      "   ..., \n",
      "   [ 0.44556057  0.2104693   0.28916441]\n",
      "   [ 0.61423935  0.39506338  0.10325649]\n",
      "   [ 0.05635458  0.8378674   0.85790131]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.35559305  0.2532878   0.23764048]\n",
      "   [ 0.42841687  0.13936585  0.89868522]\n",
      "   [ 0.78594937  0.38990995  0.67079933]\n",
      "   ..., \n",
      "   [ 0.53712862  0.43112461  0.44864514]\n",
      "   [ 0.08828806  0.92606751  0.26186818]\n",
      "   [ 0.37303936  0.55557797  0.27560589]]\n",
      "\n",
      "  [[ 0.08326403  0.48991489  0.26005135]\n",
      "   [ 0.05362985  0.11611568  0.34613373]\n",
      "   [ 0.85928028  0.21763345  0.40876147]\n",
      "   ..., \n",
      "   [ 0.12242801  0.28171471  0.93687373]\n",
      "   [ 0.41735005  0.37833438  0.82231001]\n",
      "   [ 0.71642238  0.83353079  0.24634212]]\n",
      "\n",
      "  [[ 0.54824395  0.56962862  0.06891168]\n",
      "   [ 0.53255951  0.91095508  0.77589783]\n",
      "   [ 0.64646244  0.74971246  0.09215312]\n",
      "   ..., \n",
      "   [ 0.36598604  0.53302618  0.84408342]\n",
      "   [ 0.40622289  0.60781348  0.41729131]\n",
      "   [ 0.30386978  0.67739903  0.59414912]]]\n",
      "\n",
      "\n",
      " [[[ 0.88274424  0.02399948  0.02629562]\n",
      "   [ 0.82710149  0.50173467  0.45150911]\n",
      "   [ 0.40901982  0.12309492  0.27736051]\n",
      "   ..., \n",
      "   [ 0.11524333  0.35668449  0.45209988]\n",
      "   [ 0.32590078  0.73659747  0.69892541]\n",
      "   [ 0.43838203  0.8318257   0.61892768]]\n",
      "\n",
      "  [[ 0.28553966  0.2179838   0.47324181]\n",
      "   [ 0.97320203  0.43691308  0.67871318]\n",
      "   [ 0.64637588  0.02574167  0.33516544]\n",
      "   ..., \n",
      "   [ 0.5242239   0.53336887  0.52630098]\n",
      "   [ 0.73084741  0.31284793  0.48482747]\n",
      "   [ 0.75942426  0.62618971  0.74033223]]\n",
      "\n",
      "  [[ 0.89403522  0.02604441  0.82203309]\n",
      "   [ 0.96890783  0.86418313  0.71770474]\n",
      "   [ 0.94145048  0.11889863  0.21797015]\n",
      "   ..., \n",
      "   [ 0.0502407   0.63553844  0.67571512]\n",
      "   [ 0.13034225  0.71025762  0.99836372]\n",
      "   [ 0.31973748  0.47999542  0.17910893]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.22902299  0.06848555  0.15111653]\n",
      "   [ 0.52038425  0.86114041  0.14752943]\n",
      "   [ 0.41828817  0.06077876  0.28896544]\n",
      "   ..., \n",
      "   [ 0.52816731  0.29105633  0.56733978]\n",
      "   [ 0.03696933  0.76005423  0.76377788]\n",
      "   [ 0.02463461  0.40862892  0.77253521]]\n",
      "\n",
      "  [[ 0.07297387  0.424982    0.8460481 ]\n",
      "   [ 0.00410954  0.32882348  0.70660933]\n",
      "   [ 0.41683655  0.46958526  0.70564526]\n",
      "   ..., \n",
      "   [ 0.12132926  0.27298851  0.94872474]\n",
      "   [ 0.37576247  0.15606737  0.68033541]\n",
      "   [ 0.76717145  0.22704045  0.06119515]]\n",
      "\n",
      "  [[ 0.23819345  0.95635452  0.96787341]\n",
      "   [ 0.23377592  0.04329008  0.6891789 ]\n",
      "   [ 0.75104571  0.59163421  0.87014691]\n",
      "   ..., \n",
      "   [ 0.32905597  0.73147808  0.46486226]\n",
      "   [ 0.38605523  0.97667538  0.55382501]\n",
      "   [ 0.01640734  0.70310264  0.37953573]]]\n",
      "\n",
      "\n",
      " [[[ 0.40499134  0.54613108  0.98697157]\n",
      "   [ 0.9250621   0.46641493  0.94335445]\n",
      "   [ 0.7204756   0.89104478  0.15928371]\n",
      "   ..., \n",
      "   [ 0.22687765  0.00510656  0.13802338]\n",
      "   [ 0.52443357  0.04293373  0.74029294]\n",
      "   [ 0.39816639  0.03725989  0.91076933]]\n",
      "\n",
      "  [[ 0.09587662  0.73995349  0.33980523]\n",
      "   [ 0.77766697  0.96737908  0.19867574]\n",
      "   [ 0.07512513  0.17944275  0.43765856]\n",
      "   ..., \n",
      "   [ 0.79920025  0.64078905  0.91249257]\n",
      "   [ 0.31086567  0.6917254   0.95056942]\n",
      "   [ 0.97145246  0.8216759   0.71391267]]\n",
      "\n",
      "  [[ 0.52154837  0.39153968  0.85554742]\n",
      "   [ 0.51704347  0.68654975  0.25865712]\n",
      "   [ 0.23412339  0.07927483  0.88004589]\n",
      "   ..., \n",
      "   [ 0.90656385  0.29681202  0.16268782]\n",
      "   [ 0.09027267  0.84453145  0.95586309]\n",
      "   [ 0.85606161  0.80090289  0.27166927]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.92316378  0.99210134  0.41871401]\n",
      "   [ 0.04792108  0.72821173  0.12419099]\n",
      "   [ 0.06013985  0.26807873  0.73734534]\n",
      "   ..., \n",
      "   [ 0.19819013  0.17087741  0.78860424]\n",
      "   [ 0.35578013  0.44940406  0.00647532]\n",
      "   [ 0.57819679  0.98417962  0.94810956]]\n",
      "\n",
      "  [[ 0.04223389  0.77207011  0.17231387]\n",
      "   [ 0.93665136  0.50526035  0.38879218]\n",
      "   [ 0.69152291  0.33838411  0.59584927]\n",
      "   ..., \n",
      "   [ 0.78242947  0.45743778  0.66002739]\n",
      "   [ 0.25240827  0.7551096   0.1066585 ]\n",
      "   [ 0.39407972  0.71875231  0.62189399]]\n",
      "\n",
      "  [[ 0.43345021  0.17540632  0.7013812 ]\n",
      "   [ 0.2278953   0.50941714  0.00885312]\n",
      "   [ 0.96662162  0.38801344  0.59728334]\n",
      "   ..., \n",
      "   [ 0.4277974   0.05493384  0.11697716]\n",
      "   [ 0.22005747  0.32000407  0.21547925]\n",
      "   [ 0.97405559  0.24993946  0.44878461]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.0488813   0.95970453  0.42939876]\n",
      "   [ 0.3846705   0.54211606  0.06466149]\n",
      "   [ 0.54781791  0.68748377  0.04805289]\n",
      "   ..., \n",
      "   [ 0.81373074  0.27221491  0.1387214 ]\n",
      "   [ 0.64758333  0.65612444  0.16422644]\n",
      "   [ 0.77118952  0.93295053  0.9377628 ]]\n",
      "\n",
      "  [[ 0.75118504  0.44653729  0.2577092 ]\n",
      "   [ 0.21798504  0.67797537  0.41737016]\n",
      "   [ 0.3997293   0.61944048  0.45302575]\n",
      "   ..., \n",
      "   [ 0.27905064  0.80536177  0.1401152 ]\n",
      "   [ 0.91915443  0.96969058  0.63905121]\n",
      "   [ 0.4994799   0.80281301  0.50998584]]\n",
      "\n",
      "  [[ 0.73214232  0.72675063  0.35322662]\n",
      "   [ 0.65420664  0.04692111  0.30167921]\n",
      "   [ 0.87790993  0.63027371  0.02325952]\n",
      "   ..., \n",
      "   [ 0.96723339  0.68970739  0.35208709]\n",
      "   [ 0.24703221  0.93461491  0.52985898]\n",
      "   [ 0.535746    0.0366908   0.74282937]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.919274    0.18736196  0.39438442]\n",
      "   [ 0.29733962  0.9351134   0.81828415]\n",
      "   [ 0.3566067   0.44625554  0.19070472]\n",
      "   ..., \n",
      "   [ 0.66599213  0.99469465  0.84337505]\n",
      "   [ 0.72030303  0.65879584  0.04989383]\n",
      "   [ 0.08463176  0.59207801  0.73743301]]\n",
      "\n",
      "  [[ 0.28385257  0.00877317  0.23314464]\n",
      "   [ 0.78899999  0.24322363  0.38258359]\n",
      "   [ 0.83836163  0.551755    0.02712574]\n",
      "   ..., \n",
      "   [ 0.47266104  0.47833771  0.6646214 ]\n",
      "   [ 0.95458628  0.89496496  0.23413825]\n",
      "   [ 0.74480594  0.85999805  0.36513515]]\n",
      "\n",
      "  [[ 0.61851287  0.42612851  0.3487443 ]\n",
      "   [ 0.32907394  0.79463366  0.20218422]\n",
      "   [ 0.53707433  0.10809547  0.30279441]\n",
      "   ..., \n",
      "   [ 0.70430473  0.8182581   0.95834344]\n",
      "   [ 0.87399688  0.5557224   0.87305218]\n",
      "   [ 0.91653046  0.91039655  0.45309976]]]\n",
      "\n",
      "\n",
      " [[[ 0.39639592  0.9624629   0.12282942]\n",
      "   [ 0.68982181  0.64982282  0.60055049]\n",
      "   [ 0.52468947  0.29974925  0.7883978 ]\n",
      "   ..., \n",
      "   [ 0.58412905  0.15370892  0.70650363]\n",
      "   [ 0.57126184  0.26300843  0.94724055]\n",
      "   [ 0.50943944  0.17668166  0.08344664]]\n",
      "\n",
      "  [[ 0.59405487  0.81957543  0.56468982]\n",
      "   [ 0.25261984  0.53063697  0.34214492]\n",
      "   [ 0.84882755  0.8254378   0.18934111]\n",
      "   ..., \n",
      "   [ 0.59120745  0.80566531  0.51058234]\n",
      "   [ 0.50599204  0.00224654  0.34666276]\n",
      "   [ 0.06685805  0.32769807  0.29397813]]\n",
      "\n",
      "  [[ 0.24637333  0.93253744  0.82794756]\n",
      "   [ 0.15484896  0.29050919  0.14806473]\n",
      "   [ 0.15271591  0.84904038  0.08827102]\n",
      "   ..., \n",
      "   [ 0.58318981  0.47174213  0.83783633]\n",
      "   [ 0.1598787   0.54219664  0.95429036]\n",
      "   [ 0.77167078  0.68816122  0.98754215]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.72332279  0.50265696  0.26065939]\n",
      "   [ 0.8756259   0.43826482  0.28911131]\n",
      "   [ 0.6107929   0.1557335   0.55695975]\n",
      "   ..., \n",
      "   [ 0.9009708   0.76816285  0.61904788]\n",
      "   [ 0.65920649  0.16671797  0.38691244]\n",
      "   [ 0.65852312  0.04578705  0.30893786]]\n",
      "\n",
      "  [[ 0.83533647  0.94839948  0.51700385]\n",
      "   [ 0.80034125  0.00371966  0.85151718]\n",
      "   [ 0.75323877  0.54603429  0.46704678]\n",
      "   ..., \n",
      "   [ 0.4912585   0.16547631  0.8174736 ]\n",
      "   [ 0.62028148  0.20432768  0.39268983]\n",
      "   [ 0.13040412  0.93195885  0.97626171]]\n",
      "\n",
      "  [[ 0.46340526  0.41786084  0.11272829]\n",
      "   [ 0.81198478  0.65088173  0.81206055]\n",
      "   [ 0.81587011  0.4795067   0.14685154]\n",
      "   ..., \n",
      "   [ 0.24537628  0.8133256   0.9948132 ]\n",
      "   [ 0.48863938  0.40384584  0.15382165]\n",
      "   [ 0.34307248  0.91573739  0.80990695]]]\n",
      "\n",
      "\n",
      " [[[ 0.56879655  0.56655547  0.12569927]\n",
      "   [ 0.7984048   0.25989916  0.69866432]\n",
      "   [ 0.91649975  0.93382426  0.80169673]\n",
      "   ..., \n",
      "   [ 0.48905794  0.66742632  0.57137357]\n",
      "   [ 0.71595307  0.20710031  0.7936401 ]\n",
      "   [ 0.36541664  0.14832634  0.85587996]]\n",
      "\n",
      "  [[ 0.82232367  0.39808641  0.53898961]\n",
      "   [ 0.73323943  0.67826171  0.22794393]\n",
      "   [ 0.9911504   0.79638015  0.92430087]\n",
      "   ..., \n",
      "   [ 0.59535577  0.50760592  0.28452604]\n",
      "   [ 0.34167254  0.23513278  0.51245946]\n",
      "   [ 0.15159435  0.5329099   0.72917513]]\n",
      "\n",
      "  [[ 0.11401223  0.61223581  0.13253819]\n",
      "   [ 0.59445794  0.13675914  0.29484501]\n",
      "   [ 0.1381638   0.0652414   0.55711932]\n",
      "   ..., \n",
      "   [ 0.15067942  0.64309327  0.15378411]\n",
      "   [ 0.61779678  0.05521091  0.95521692]\n",
      "   [ 0.35021227  0.01417389  0.06813387]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.6290867   0.30060611  0.4778255 ]\n",
      "   [ 0.25848517  0.57086608  0.81057614]\n",
      "   [ 0.30595025  0.15332499  0.61424907]\n",
      "   ..., \n",
      "   [ 0.83984768  0.46219947  0.20287841]\n",
      "   [ 0.86679529  0.74998449  0.37374436]\n",
      "   [ 0.23247643  0.33582608  0.65090225]]\n",
      "\n",
      "  [[ 0.02554341  0.91421059  0.78656243]\n",
      "   [ 0.73809365  0.06578349  0.50113074]\n",
      "   [ 0.80037701  0.67632634  0.88717817]\n",
      "   ..., \n",
      "   [ 0.82484094  0.05572901  0.45175604]\n",
      "   [ 0.97395449  0.2728112   0.43643421]\n",
      "   [ 0.74386073  0.85732652  0.42709189]]\n",
      "\n",
      "  [[ 0.62065985  0.24426862  0.3555111 ]\n",
      "   [ 0.28067689  0.10926964  0.81925591]\n",
      "   [ 0.51551693  0.49260276  0.89671501]\n",
      "   ..., \n",
      "   [ 0.48087613  0.61649088  0.35250429]\n",
      "   [ 0.71260609  0.00442435  0.6372    ]\n",
      "   [ 0.51316086  0.3067912   0.01497843]]]]\n",
      "[array([[[[ 0.89832491,  0.74073261,  0.69702154],\n",
      "         [ 0.41813198,  0.30042931,  0.55091661],\n",
      "         [ 0.17186376,  0.71949196,  0.75365478],\n",
      "         ..., \n",
      "         [ 0.81863701,  0.56082666,  0.70380348],\n",
      "         [ 0.40530723,  0.94762403,  0.66999269],\n",
      "         [ 0.12545994,  0.70678031,  0.86147225]],\n",
      "\n",
      "        [[ 0.47938645,  0.66544563,  0.02400668],\n",
      "         [ 0.00351703,  0.43009898,  0.35642967],\n",
      "         [ 0.87072867,  0.64561301,  0.33936733],\n",
      "         ..., \n",
      "         [ 0.13518703,  0.75539666,  0.66291678],\n",
      "         [ 0.24880344,  0.71103227,  0.20449154],\n",
      "         [ 0.01424011,  0.55126727,  0.62524348]],\n",
      "\n",
      "        [[ 0.24424882,  0.45313096,  0.58476979],\n",
      "         [ 0.86441356,  0.46237934,  0.8573848 ],\n",
      "         [ 0.7085098 ,  0.87347841,  0.68618125],\n",
      "         ..., \n",
      "         [ 0.44556057,  0.21046931,  0.28916439],\n",
      "         [ 0.61423934,  0.39506337,  0.10325649],\n",
      "         [ 0.05635459,  0.83786738,  0.85790133]],\n",
      "\n",
      "        ..., \n",
      "        [[ 0.35559306,  0.25328779,  0.23764049],\n",
      "         [ 0.42841688,  0.13936585,  0.89868522],\n",
      "         [ 0.78594935,  0.38990995,  0.67079931],\n",
      "         ..., \n",
      "         [ 0.53712863,  0.4311246 ,  0.44864514],\n",
      "         [ 0.08828806,  0.92606753,  0.26186818],\n",
      "         [ 0.37303936,  0.55557799,  0.27560589]],\n",
      "\n",
      "        [[ 0.08326402,  0.48991489,  0.26005134],\n",
      "         [ 0.05362985,  0.11611568,  0.34613374],\n",
      "         [ 0.85928029,  0.21763344,  0.40876147],\n",
      "         ..., \n",
      "         [ 0.12242801,  0.28171471,  0.93687373],\n",
      "         [ 0.41735005,  0.37833437,  0.82231003],\n",
      "         [ 0.71642238,  0.83353078,  0.24634212]],\n",
      "\n",
      "        [[ 0.54824394,  0.5696286 ,  0.06891169],\n",
      "         [ 0.53255951,  0.91095507,  0.77589786],\n",
      "         [ 0.64646244,  0.74971247,  0.09215312],\n",
      "         ..., \n",
      "         [ 0.36598605,  0.53302616,  0.84408343],\n",
      "         [ 0.40622288,  0.60781348,  0.41729131],\n",
      "         [ 0.30386978,  0.67739904,  0.59414911]]],\n",
      "\n",
      "\n",
      "       [[[ 0.88274425,  0.02399947,  0.02629562],\n",
      "         [ 0.82710147,  0.50173467,  0.45150912],\n",
      "         [ 0.40901983,  0.12309492,  0.2773605 ],\n",
      "         ..., \n",
      "         [ 0.11524333,  0.35668451,  0.45209989],\n",
      "         [ 0.32590076,  0.73659748,  0.69892538],\n",
      "         [ 0.43838203,  0.83182567,  0.61892766]],\n",
      "\n",
      "        [[ 0.28553966,  0.2179838 ,  0.47324181],\n",
      "         [ 0.97320205,  0.43691307,  0.6787132 ],\n",
      "         [ 0.64637589,  0.02574167,  0.33516544],\n",
      "         ..., \n",
      "         [ 0.52422392,  0.53336889,  0.52630097],\n",
      "         [ 0.73084742,  0.31284794,  0.48482746],\n",
      "         [ 0.75942427,  0.62618971,  0.74033225]],\n",
      "\n",
      "        [[ 0.89403522,  0.02604442,  0.82203311],\n",
      "         [ 0.96890783,  0.86418313,  0.71770471],\n",
      "         [ 0.94145048,  0.11889863,  0.21797015],\n",
      "         ..., \n",
      "         [ 0.0502407 ,  0.63553846,  0.67571509],\n",
      "         [ 0.13034225,  0.71025759,  0.99836373],\n",
      "         [ 0.31973746,  0.47999543,  0.17910893]],\n",
      "\n",
      "        ..., \n",
      "        [[ 0.22902299,  0.06848555,  0.15111654],\n",
      "         [ 0.52038425,  0.86114043,  0.14752944],\n",
      "         [ 0.41828817,  0.06077876,  0.28896543],\n",
      "         ..., \n",
      "         [ 0.52816731,  0.29105633,  0.56733978],\n",
      "         [ 0.03696933,  0.76005423,  0.76377785],\n",
      "         [ 0.02463461,  0.40862891,  0.7725352 ]],\n",
      "\n",
      "        [[ 0.07297387,  0.42498201,  0.84604812],\n",
      "         [ 0.00410954,  0.32882348,  0.70660931],\n",
      "         [ 0.41683656,  0.46958527,  0.70564526],\n",
      "         ..., \n",
      "         [ 0.12132926,  0.2729885 ,  0.94872475],\n",
      "         [ 0.37576246,  0.15606737,  0.6803354 ],\n",
      "         [ 0.76717144,  0.22704045,  0.06119515]],\n",
      "\n",
      "        [[ 0.23819345,  0.9563545 ,  0.96787339],\n",
      "         [ 0.23377591,  0.04329008,  0.68917888],\n",
      "         [ 0.7510457 ,  0.59163421,  0.87014693],\n",
      "         ..., \n",
      "         [ 0.32905596,  0.7314781 ,  0.46486226],\n",
      "         [ 0.38605523,  0.97667539,  0.55382502],\n",
      "         [ 0.01640734,  0.70310265,  0.37953573]]],\n",
      "\n",
      "\n",
      "       [[[ 0.40499133,  0.54613107,  0.98697156],\n",
      "         [ 0.92506212,  0.46641493,  0.94335443],\n",
      "         [ 0.72047561,  0.8910448 ,  0.15928371],\n",
      "         ..., \n",
      "         [ 0.22687764,  0.00510656,  0.13802338],\n",
      "         [ 0.52443355,  0.04293373,  0.74029297],\n",
      "         [ 0.39816639,  0.03725988,  0.91076934]],\n",
      "\n",
      "        [[ 0.09587662,  0.73995352,  0.33980525],\n",
      "         [ 0.77766699,  0.96737909,  0.19867574],\n",
      "         [ 0.07512513,  0.17944275,  0.43765855],\n",
      "         ..., \n",
      "         [ 0.79920024,  0.64078903,  0.91249257],\n",
      "         [ 0.31086567,  0.69172537,  0.95056939],\n",
      "         [ 0.97145247,  0.8216759 ,  0.71391267]],\n",
      "\n",
      "        [[ 0.52154839,  0.39153966,  0.85554743],\n",
      "         [ 0.51704347,  0.68654972,  0.25865713],\n",
      "         [ 0.23412339,  0.07927483,  0.88004589],\n",
      "         ..., \n",
      "         [ 0.90656388,  0.29681203,  0.16268782],\n",
      "         [ 0.09027267,  0.84453148,  0.95586306],\n",
      "         [ 0.85606164,  0.8009029 ,  0.27166927]],\n",
      "\n",
      "        ..., \n",
      "        [[ 0.92316377,  0.99210131,  0.41871402],\n",
      "         [ 0.04792108,  0.72821176,  0.12419099],\n",
      "         [ 0.06013985,  0.26807871,  0.73734534],\n",
      "         ..., \n",
      "         [ 0.19819014,  0.17087741,  0.78860426],\n",
      "         [ 0.35578012,  0.44940406,  0.00647532],\n",
      "         [ 0.57819676,  0.98417962,  0.94810957]],\n",
      "\n",
      "        [[ 0.04223389,  0.77207011,  0.17231387],\n",
      "         [ 0.93665135,  0.50526035,  0.38879219],\n",
      "         [ 0.6915229 ,  0.33838412,  0.59584928],\n",
      "         ..., \n",
      "         [ 0.78242946,  0.45743778,  0.66002738],\n",
      "         [ 0.25240827,  0.75510961,  0.1066585 ],\n",
      "         [ 0.39407972,  0.71875232,  0.621894  ]],\n",
      "\n",
      "        [[ 0.43345019,  0.17540632,  0.70138121],\n",
      "         [ 0.2278953 ,  0.50941712,  0.00885312],\n",
      "         [ 0.96662164,  0.38801345,  0.59728336],\n",
      "         ..., \n",
      "         [ 0.42779741,  0.05493384,  0.11697716],\n",
      "         [ 0.22005747,  0.32000408,  0.21547925],\n",
      "         [ 0.97405559,  0.24993946,  0.44878462]]],\n",
      "\n",
      "\n",
      "       ..., \n",
      "       [[[ 0.0488813 ,  0.95970452,  0.42939875],\n",
      "         [ 0.3846705 ,  0.54211605,  0.06466149],\n",
      "         [ 0.54781789,  0.68748379,  0.04805288],\n",
      "         ..., \n",
      "         [ 0.81373072,  0.27221492,  0.13872139],\n",
      "         [ 0.64758331,  0.65612441,  0.16422644],\n",
      "         [ 0.77118951,  0.93295056,  0.9377628 ]],\n",
      "\n",
      "        [[ 0.75118506,  0.44653729,  0.25770921],\n",
      "         [ 0.21798505,  0.67797536,  0.41737017],\n",
      "         [ 0.39972931,  0.6194405 ,  0.45302576],\n",
      "         ..., \n",
      "         [ 0.27905065,  0.80536175,  0.1401152 ],\n",
      "         [ 0.91915441,  0.96969056,  0.6390512 ],\n",
      "         [ 0.49947989,  0.80281299,  0.50998586]],\n",
      "\n",
      "        [[ 0.73214233,  0.72675061,  0.3532266 ],\n",
      "         [ 0.65420663,  0.04692111,  0.30167919],\n",
      "         [ 0.87790996,  0.6302737 ,  0.02325952],\n",
      "         ..., \n",
      "         [ 0.96723336,  0.6897074 ,  0.35208708],\n",
      "         [ 0.24703221,  0.9346149 ,  0.52985901],\n",
      "         [ 0.53574598,  0.0366908 ,  0.74282938]],\n",
      "\n",
      "        ..., \n",
      "        [[ 0.91927397,  0.18736196,  0.39438441],\n",
      "         [ 0.29733962,  0.93511343,  0.81828415],\n",
      "         [ 0.35660669,  0.44625553,  0.19070472],\n",
      "         ..., \n",
      "         [ 0.66599214,  0.99469465,  0.84337503],\n",
      "         [ 0.720303  ,  0.65879583,  0.04989383],\n",
      "         [ 0.08463176,  0.59207803,  0.73743302]],\n",
      "\n",
      "        [[ 0.28385258,  0.00877317,  0.23314464],\n",
      "         [ 0.78899997,  0.24322364,  0.38258359],\n",
      "         [ 0.83836162,  0.55175501,  0.02712574],\n",
      "         ..., \n",
      "         [ 0.47266105,  0.47833771,  0.66462141],\n",
      "         [ 0.95458627,  0.89496493,  0.23413825],\n",
      "         [ 0.74480593,  0.85999805,  0.36513513]],\n",
      "\n",
      "        [[ 0.61851287,  0.42612851,  0.3487443 ],\n",
      "         [ 0.32907394,  0.79463363,  0.20218422],\n",
      "         [ 0.53707433,  0.10809547,  0.3027944 ],\n",
      "         ..., \n",
      "         [ 0.70430475,  0.81825811,  0.95834345],\n",
      "         [ 0.87399685,  0.55572242,  0.87305218],\n",
      "         [ 0.91653049,  0.91039658,  0.45309976]]],\n",
      "\n",
      "\n",
      "       [[[ 0.39639592,  0.9624629 ,  0.12282941],\n",
      "         [ 0.68982184,  0.64982283,  0.60055047],\n",
      "         [ 0.5246895 ,  0.29974926,  0.78839779],\n",
      "         ..., \n",
      "         [ 0.58412904,  0.15370892,  0.70650363],\n",
      "         [ 0.57126182,  0.26300842,  0.94724053],\n",
      "         [ 0.50943947,  0.17668167,  0.08344664]],\n",
      "\n",
      "        [[ 0.59405488,  0.81957543,  0.56468982],\n",
      "         [ 0.25261983,  0.53063697,  0.34214491],\n",
      "         [ 0.84882754,  0.82543778,  0.18934111],\n",
      "         ..., \n",
      "         [ 0.59120744,  0.80566531,  0.51058233],\n",
      "         [ 0.50599205,  0.00224654,  0.34666276],\n",
      "         [ 0.06685805,  0.32769808,  0.29397812]],\n",
      "\n",
      "        [[ 0.24637333,  0.93253744,  0.82794756],\n",
      "         [ 0.15484896,  0.29050919,  0.14806473],\n",
      "         [ 0.15271591,  0.84904039,  0.08827101],\n",
      "         ..., \n",
      "         [ 0.58318979,  0.47174212,  0.83783633],\n",
      "         [ 0.1598787 ,  0.54219663,  0.95429033],\n",
      "         [ 0.77167076,  0.68816119,  0.98754215]],\n",
      "\n",
      "        ..., \n",
      "        [[ 0.72332281,  0.50265694,  0.2606594 ],\n",
      "         [ 0.87562591,  0.43826482,  0.28911132],\n",
      "         [ 0.61079288,  0.1557335 ,  0.55695975],\n",
      "         ..., \n",
      "         [ 0.90097082,  0.76816285,  0.61904788],\n",
      "         [ 0.65920651,  0.16671798,  0.38691244],\n",
      "         [ 0.65852314,  0.04578705,  0.30893788]],\n",
      "\n",
      "        [[ 0.83533645,  0.94839948,  0.51700383],\n",
      "         [ 0.80034125,  0.00371966,  0.8515172 ],\n",
      "         [ 0.7532388 ,  0.54603428,  0.46704677],\n",
      "         ..., \n",
      "         [ 0.4912585 ,  0.16547631,  0.81747359],\n",
      "         [ 0.62028146,  0.20432767,  0.39268982],\n",
      "         [ 0.13040413,  0.93195885,  0.97626173]],\n",
      "\n",
      "        [[ 0.46340525,  0.41786084,  0.11272828],\n",
      "         [ 0.81198478,  0.65088171,  0.81206053],\n",
      "         [ 0.81587011,  0.4795067 ,  0.14685154],\n",
      "         ..., \n",
      "         [ 0.24537629,  0.81332558,  0.9948132 ],\n",
      "         [ 0.48863938,  0.40384585,  0.15382165],\n",
      "         [ 0.34307247,  0.91573739,  0.80990696]]],\n",
      "\n",
      "\n",
      "       [[[ 0.56879658,  0.56655544,  0.12569927],\n",
      "         [ 0.79840481,  0.25989917,  0.69866431],\n",
      "         [ 0.91649973,  0.93382424,  0.80169672],\n",
      "         ..., \n",
      "         [ 0.48905793,  0.66742635,  0.57137358],\n",
      "         [ 0.71595305,  0.20710032,  0.79364008],\n",
      "         [ 0.36541665,  0.14832634,  0.85587996]],\n",
      "\n",
      "        [[ 0.82232368,  0.3980864 ,  0.5389896 ],\n",
      "         [ 0.73323941,  0.6782617 ,  0.22794393],\n",
      "         [ 0.99115038,  0.79638016,  0.92430085],\n",
      "         ..., \n",
      "         [ 0.59535575,  0.50760591,  0.28452605],\n",
      "         [ 0.34167254,  0.23513278,  0.51245946],\n",
      "         [ 0.15159436,  0.53290993,  0.72917515]],\n",
      "\n",
      "        [[ 0.11401223,  0.61223578,  0.13253818],\n",
      "         [ 0.59445792,  0.13675913,  0.29484501],\n",
      "         [ 0.13816381,  0.0652414 ,  0.55711931],\n",
      "         ..., \n",
      "         [ 0.15067942,  0.64309329,  0.15378411],\n",
      "         [ 0.61779678,  0.05521091,  0.95521694],\n",
      "         [ 0.35021228,  0.01417389,  0.06813387]],\n",
      "\n",
      "        ..., \n",
      "        [[ 0.62908667,  0.3006061 ,  0.47782549],\n",
      "         [ 0.25848517,  0.57086605,  0.81057614],\n",
      "         [ 0.30595025,  0.15332499,  0.61424905],\n",
      "         ..., \n",
      "         [ 0.83984768,  0.46219948,  0.2028784 ],\n",
      "         [ 0.8667953 ,  0.7499845 ,  0.37374437],\n",
      "         [ 0.23247643,  0.33582607,  0.65090227]],\n",
      "\n",
      "        [[ 0.02554341,  0.91421062,  0.78656244],\n",
      "         [ 0.73809367,  0.06578349,  0.50113076],\n",
      "         [ 0.80037701,  0.67632633,  0.88717818],\n",
      "         ..., \n",
      "         [ 0.82484096,  0.05572901,  0.45175603],\n",
      "         [ 0.9739545 ,  0.2728112 ,  0.43643421],\n",
      "         [ 0.74386072,  0.85732651,  0.4270919 ]],\n",
      "\n",
      "        [[ 0.62065983,  0.24426861,  0.3555111 ],\n",
      "         [ 0.2806769 ,  0.10926964,  0.81925589],\n",
      "         [ 0.51551694,  0.49260277,  0.89671499],\n",
      "         ..., \n",
      "         [ 0.48087612,  0.6164909 ,  0.35250428],\n",
      "         [ 0.71260607,  0.00442435,  0.6372    ],\n",
      "         [ 0.51316088,  0.30679119,  0.01497843]]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.random(size=(10,30,30,3))\n",
    "print (x.shape)\n",
    "\n",
    "tf_x = tf.placeholder(\"float\", [None, 30,30,3])\n",
    "\n",
    "input1 = tf.placeholder(tf.float32) # place holder is not able to be eval\n",
    "input2 = tf.Variable(x)\n",
    "#output = tf.mul(input1, input2)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    print (input2.eval())\n",
    "    \n",
    "    # Show in list or np array \n",
    "    x_ = sess.run([tf_x], feed_dict = { tf_x:x })\n",
    "    print (x_ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
