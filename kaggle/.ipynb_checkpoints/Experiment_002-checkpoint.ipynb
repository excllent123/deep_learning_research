{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memo\n",
    "- small data sample with many features \n",
    "- approach with traditional machine learning \n",
    "\n",
    "### Key Element \n",
    "- BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "- ElasticNetCV, LassoLarsCV\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import ElasticNetCV, LassoLarsCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X1~X8 is categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "id_test = test['ID']\n",
    "train.pop('ID')\n",
    "test.pop('ID')\n",
    "\n",
    "for c in train.columns:\n",
    "    if train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[c].values) + list(test[c].values))\n",
    "        train[c] = lbl.transform(list(train[c].values))\n",
    "        test[c] = lbl.transform(list(test[c].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kentchiu/anaconda/lib/python3.6/site-packages/sklearn/decomposition/fastica_.py:116: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    }
   ],
   "source": [
    "n_comp = 12\n",
    "\n",
    "# tSVD\n",
    "tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "tsvd_results_train = tsvd.fit_transform(train.drop([\"y\"], axis=1))\n",
    "tsvd_results_test = tsvd.transform(test)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=n_comp, random_state=420)\n",
    "pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n",
    "pca2_results_test = pca.transform(test)\n",
    "\n",
    "# ICA\n",
    "ica = FastICA(n_components=n_comp, random_state=420)\n",
    "ica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\n",
    "ica2_results_test = ica.transform(test)\n",
    "\n",
    "# GRP\n",
    "grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "grp_results_train = grp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "grp_results_test = grp.transform(test)\n",
    "\n",
    "# SRP [Might cause -inf]\n",
    "# srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "# srp_results_train = srp.fit_transform(train.drop([\"y\"], axis=1))\n",
    "# srp_results_test = srp.transform(test)\n",
    "\n",
    "#save columns list before adding the decomposition components\n",
    "\n",
    "usable_columns = list(set(train.columns) - set(['y']))\n",
    "\n",
    "# Append decomposition components to datasets\n",
    "for i in range(1, n_comp + 1):\n",
    "    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n",
    "    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n",
    "\n",
    "    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n",
    "    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n",
    "\n",
    "    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "\n",
    "    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "\n",
    "#    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "#    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_y = train['y'].values\n",
    "usable_columns = list(set(train.columns) - set(['y']))\n",
    "\n",
    "\n",
    "feature_cols =[]\n",
    "for i in range(1, n_comp + 1):\n",
    "    feature_cols+=['pca_' + str(i), 'ica_' + str(i), 'tsvd_' + str(i), \n",
    "                        'grp_' + str(i)]\n",
    "\n",
    "#feature_cols = usable_columns \n",
    "# use only projected features\n",
    "train_x = train[feature_cols].copy()\n",
    "test   = test[feature_cols].copy()\n",
    "\n",
    "# zero-center and normalized with train_x and dtest\n",
    "\n",
    "\n",
    "A = train_x.mean()\n",
    "B = 1/train_x.max()\n",
    "train_x -= A\n",
    "train_x *= B\n",
    "\n",
    "test -= A \n",
    "test *= B\n",
    "\n",
    "\n",
    "# normalized y (aX+b)\n",
    "b = train_y.mean()\n",
    "a = 1/train_y.max()\n",
    "train_y -= b\n",
    "train_y *= a\n",
    "\n",
    "def recover_y(value, a, b):\n",
    "    value*=(1/a)\n",
    "    value+= b\n",
    "    return value \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "'''Train the xgb model then predict the test data'''\n",
    "\n",
    "xgb_params = {'n_trees': 400, \n",
    "    'eta': 0.0045,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.9,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1}\n",
    "# NOTE: Make sure that the class is labeled 'class' in the data file\n",
    "\n",
    "dtrain = xgb.DMatrix(train_x, train_y)\n",
    "dtest = xgb.DMatrix(test)\n",
    "\n",
    "num_boost_rounds = 1250\n",
    "\n",
    "# train model\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "y_pred = model.predict(dtest)\n",
    "y_pred = list(recover_y(y_pred, a, b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 71.34112\n",
      "8 12 109.30903\n",
      "17 23 115.21953\n",
      "19 28 92.00675\n",
      "24 42 87.73572\n",
      "25 43 129.79876\n",
      "26 45 99.55671\n",
      "32 57 116.02167\n",
      "1985 3977 132.08556\n"
     ]
    }
   ],
   "source": [
    "#### Value Replacing with ill-conditioned in test data\n",
    "# https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion/35271\n",
    "# https://crowdstats.eu/topics/kaggle-mercedes-benz-greener-manufacturing-leaderboard-probing\n",
    "memo = { 1 : 71.34112, 12 : 109.30903, 23 : 115.21953, 28 : 92.00675, 42 : 87.73572, 43 : 129.79876, \n",
    "        45 : 99.55671, 57 : 116.02167, 3977 : 132.08556}\n",
    "for i in range(len(id_test)):\n",
    "    if id_test[i] in memo.keys():\n",
    "        y_pred[i] = memo[id_test[i]]\n",
    "        print (i, id_test[i], memo[id_test[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel_11.csv \\nxgb_params = {\\n    'n_trees': 320, \\n    'eta': 0.0045,\\n    'max_depth': 5,\\n    'subsample': 0.9,\\n    'objective': 'reg:linear',\\n    'eval_metric': 'rmse',\\n    'base_score': y_mean, # base prediction = mean(target)\\n    'silent': 1}\\n    \\nmodel_12.csv \\nsub feature cols to unsample_col \\n\\nmodel_13.csv\\nuse feature cols\\nxgb_params = {'n_trees': 400, \\n    'eta': 0.0045,\\n    'max_depth': 4,\\n    'subsample': 0.9,\\n    'objective': 'reg:linear',\\n    'eval_metric': 'rmse',\\n    'base_score': y_mean, # base prediction = mean(target)\\n    'silent': 1}\\n    \\nLBacc = 0.53\\n\\nmodel_14.csv\\nxgb_params = {'n_trees': 400, \\n    'eta': 0.0045,\\n    'max_depth': 5,\\n    'subsample': 0.9,\\n    'objective': 'reg:linear',\\n    'eval_metric': 'rmse',\\n    'base_score': y_mean, # base prediction = mean(target)\\n    'silent': 1}\\n\\nmodel_15.csv\\nreplce prediction outlier, ill-condition\\nxgb_params = {'n_trees': 400, \\n    'eta': 0.0045,\\n    'max_depth': 4,\\n    'subsample': 0.9,\\n    'objective': 'reg:linear',\\n    'eval_metric': 'rmse',\\n    'base_score': y_mean, # base prediction = mean(target)\\n    'silent': 1}\\n    \\n\""
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['ID'] = id_test\n",
    "sub['y'] = y_pred\n",
    "\n",
    "# sub['y'] = y_pred*0.75 + results*0.25\n",
    "sub.to_csv('model_15.csv', index=False)\n",
    "# stacked-models_04.csv = with outlier\n",
    "# stacked-models_05.csv = without outlier second StackingEstimator max_depth=5\n",
    "# stacked-models_06.csv = second StackingEstimator max_depth=3 \n",
    "# stacked-models_07.csv = sub['y'] = y_pred*0.75 + results*0.25\n",
    "# stacked-models_08.csv = n_component from 12 -> 20, second StackingEstimator max_depth=4\n",
    "# stacked-models_09.csv = sub['y'] = y_pred*0.7145 + results*0.2855\n",
    "'''\n",
    "model_11.csv \n",
    "xgb_params = {\n",
    "    'n_trees': 320, \n",
    "    'eta': 0.0045,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.9,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1}\n",
    "    \n",
    "model_12.csv \n",
    "sub feature cols to unsample_col \n",
    "\n",
    "model_13.csv\n",
    "use feature cols\n",
    "xgb_params = {'n_trees': 400, \n",
    "    'eta': 0.0045,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.9,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1}\n",
    "    \n",
    "LBacc = 0.53\n",
    "\n",
    "model_14.csv\n",
    "xgb_params = {'n_trees': 400, \n",
    "    'eta': 0.0045,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.9,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1}\n",
    "\n",
    "model_15.csv\n",
    "replce prediction outlier, ill-condition\n",
    "xgb_params = {'n_trees': 400, \n",
    "    'eta': 0.0045,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.9,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1}\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from train_dp import AgentRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 130.81   88.53   76.26 ...,  109.22   87.48  110.85]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                310       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 6,901.0\n",
      "Trainable params: 6,901.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_y = train['y'].values\n",
    "print (train_y)\n",
    "usable_columns = list(set(train.columns) - set(['y']))\n",
    "\n",
    "feature_cols =[]\n",
    "for i in range(1, n_comp + 1):\n",
    "    feature_cols+=['pca_' + str(i), 'ica_' + str(i), 'tsvd_' + str(i), \n",
    "                        'grp_' + str(i), 'srp_' + str(i) ]\n",
    "\n",
    "train_x = train[feature_cols] \n",
    "test   = test[feature_cols]\n",
    "\n",
    "agent = AgentRegressor(lr=1e-4, batch_size=2, train_x=train_x, train_y= train_y, test = test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training \n",
      "EP [5] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [10] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [15] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [20] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [25] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [30] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [35] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [40] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [45] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [50] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [55] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [60] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [65] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [70] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [75] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [80] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [85] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [90] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [95] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [100] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [105] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [110] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [115] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [120] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [125] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [130] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [135] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [140] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [145] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [150] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [155] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [160] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [165] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [170] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [175] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [180] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [185] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [190] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [195] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [200] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [205] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [210] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [215] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [220] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [225] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [230] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [235] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [240] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [245] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [250] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [255] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [260] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [265] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [270] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [275] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [280] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [285] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [290] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [295] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [300] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [305] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [310] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [315] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [320] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [325] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [330] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [335] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [340] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [345] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [350] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [355] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [360] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [365] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [370] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [375] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [380] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [385] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [390] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [395] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [400] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [405] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [410] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [415] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [420] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [425] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [430] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [435] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [440] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [445] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [450] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [455] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [460] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [465] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [470] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [475] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [480] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [485] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [490] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [495] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [500] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [505] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [510] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [515] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [520] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [525] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [530] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [535] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [540] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [545] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [550] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [555] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [560] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [565] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [570] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [575] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [580] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [585] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [590] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [595] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [600] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [605] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [610] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [615] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [620] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [625] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [630] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [635] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [640] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [645] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [650] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [655] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [660] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [665] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [670] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [675] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [680] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [685] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [690] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [695] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [700] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [705] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [710] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [715] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [720] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [725] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [730] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [735] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [740] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [745] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [750] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [755] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [760] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [765] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [770] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [775] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [780] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [785] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [790] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [795] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [800] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [805] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [810] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [815] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [820] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [825] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [830] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [835] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [840] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [845] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [850] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [855] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [860] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [865] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [870] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [875] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [880] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [885] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [890] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [895] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [900] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [905] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [910] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [915] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [920] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [925] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [930] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [935] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [940] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [945] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [950] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [955] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [960] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [965] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [970] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [975] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [980] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [985] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [990] Loss nan Predict [[ nan]\n",
      " [ nan]]\n",
      "EP [995] Loss nan Predict [[ nan]\n",
      " [ nan]]\n"
     ]
    }
   ],
   "source": [
    "# add init  # add constrain \n",
    "# not use momentum https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network\n",
    "# still meas nan => gradient explod \n",
    "# fix net  => no use\n",
    "# avoid neg-value in train => no use \n",
    "# Truncnla init => with lower std\n",
    "# simplize the model\n",
    "# in-regression problem, it is easy to explode the gradient ...\n",
    "# use adam optimizer \n",
    "# before final layer => use softmax => give up this way\n",
    "# give up normalize output => no use\n",
    "# extremely low lr with patient \n",
    "\n",
    "## Keras Way \n",
    "# agent.train(lr=1e-8, callbacks=[prediction_history])\n",
    "agent.tf_train(lr= 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4209, 100)\n"
     ]
    }
   ],
   "source": [
    "res  = agent.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "(4209, 100) (4209, 100)\n"
     ]
    }
   ],
   "source": [
    "print (res[0])\n",
    "print (train_x.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 --- -inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'srp_11'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,j in enumerate(train_x.min()): \n",
    "    if i == 54:print (i,'---',j) \n",
    "train_x.keys()[54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
