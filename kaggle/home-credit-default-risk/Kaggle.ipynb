{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kent\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import sys\n",
    "import binary_model\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf \n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_preprocess import PandasMinMaxScaler, PandasStandardScaler\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "from deep_learning_research.preprocess_toolkit import util as UT\n",
    "import keras as ks\n",
    "from keras.backend.tensorflow_backend import clip\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_rows = 1000\n",
    "tar_dir = '../../.kaggle/competitions/home-credit-default-risk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['application_test.csv',\n",
       " 'application_train.csv',\n",
       " 'bureau.csv',\n",
       " 'bureau_balance.csv',\n",
       " 'credit_card_balance.csv',\n",
       " 'HomeCredit_columns_description.csv',\n",
       " 'installments_payments.csv',\n",
       " 'POS_CASH_balance.csv',\n",
       " 'previous_application.csv',\n",
       " 'sample_submission.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(tar_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocess Schema\n",
    "- special hard code filtering \n",
    "- data preprocess \n",
    "    - data augmentation via interaction (concated, LDA)... etc \n",
    "    - data fratorization \n",
    "\n",
    "- first level feature extractering \n",
    "- second level models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null(df):\n",
    "    se = df.isnull().sum() \n",
    "    se_all = len(df)\n",
    "    ax_df = se.apply(lambda x: x/se_all)\n",
    "    ax_df = ax_df[ax_df > 0.01]\n",
    "    if len(ax_df) < 1:\n",
    "        print('No Missing Data')\n",
    "        return None\n",
    "    ax = ax_df.plot(kind='bar',figsize=(18,2),\n",
    "                    title = str(len(se) )+' Cols with '+str(len(ax_df)) + \\\n",
    "                    'contains null' + '(Sample Size {})'.format(len(df)),\n",
    "                   )\n",
    "    ax.set_xticklabels(se.index, rotation=90)\n",
    "    plt.show()\n",
    "check_null(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Proprocess with Bureau and Bureau-balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 624.85 MB\n",
      "Memory usage after optimization is: 156.21 MB\n",
      "Decreased by 75.0%\n",
      "Memory usage of dataframe is 222.62 MB\n",
      "Memory usage after optimization is: 78.57 MB\n",
      "Decreased by 64.7%\n"
     ]
    }
   ],
   "source": [
    "bureau_balance = UT.import_data(tar_dir+'bureau_balance.csv')\n",
    "temp = bureau_balance.groupby('SK_ID_BUREAU')['STATUS'].value_counts()\n",
    "temp = temp.unstack().fillna(0)\n",
    "temp = temp.reset_index()\n",
    "bureau = UT.import_data(tar_dir+'bureau.csv')\n",
    "bureau = pd.merge(bureau, temp, on='SK_ID_BUREAU', how='left')\n",
    "bureau = bureau[['SK_ID_CURR', '0', '1', '2', '3', '4', '5', 'C', 'X']]\n",
    "bureau = bureau.fillna(0)\n",
    "del temp\n",
    "del bureau_balance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356255 BEFORE\n",
      "356255 AFTER\n"
     ]
    }
   ],
   "source": [
    "bureau = bureau.groupby('SK_ID_CURR').sum().reset_index()\n",
    "df = pd.read_csv('preprocessed_data_01.csv')\n",
    "print(len(df), 'BEFORE')\n",
    "df = pd.merge(df, bureau, on='SK_ID_CURR', how='left')\n",
    "print(len(df), 'AFTER')\n",
    "df_fill = df.fillna(0)\n",
    "\n",
    "df_fill['TARGET'] = df['TARGET']\n",
    "feats = [f for f in df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV', 'index', 'index_x', 'index_y', 'Unnamed: 0']]\n",
    "#df_fill.to_csv('preprocessed_data_02.csv', index=False)\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target balance\n",
      "1 [198600] samples, 0 [282686] samples, ratio : [0.702546288107653]\n"
     ]
    }
   ],
   "source": [
    "target_balance = 1\n",
    "if target_balance : \n",
    "    print('target balance')\n",
    "    for i in range(3):\n",
    "        df_fill = df_fill.append(df_fill[df_fill['TARGET']==1])\n",
    "    \n",
    "    a = len(df_fill[df_fill['TARGET']==1])\n",
    "    b = len(df_fill[df_fill['TARGET']==0])\n",
    "    print('1 [{}] samples, 0 [{}] samples, ratio : [{}]'.format(a, b, a/b)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# binary_model.test(test_3_df, \n",
    "#                  num_folds=5, \n",
    "#                  feats=feature4, \n",
    "#                   submission_file_name='test.csv')\n",
    "# test(df_fill, 'sk001')\n",
    "\n",
    "binary_model.kfold_lightgbm(df_fill, \n",
    "                            num_folds=3,\n",
    "                            feats=feats,\n",
    "                            stratified = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Seectiotn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feats = [f for f in test_2_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV', 'index', 'index_x', 'index_y']]\n",
    "    \n",
    "# record = binary_model.boostraping(test_2_df, X_col=feats, y_col='TARGET', models=[GradientBoostingClassifier()])\n",
    "\n",
    "def feature_set(record, thre):\n",
    "    res = []\n",
    "    for k, y in record.items():\n",
    "        if y > thre:\n",
    "            if k not in [ 'index','index_x','index_y']:\n",
    "                res.append(k)\n",
    "    return res\n",
    "\n",
    "feature4 = feature_set(record, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "feats = [f for f in test_2_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV', 'index', 'index_x', 'index_y']]\n",
    "    \n",
    "binary_model.kfold_trainer(df_fill, feats=feats, \n",
    "                           model=CatBoostClassifier(iterations=3000, depth=9, thread_count=2, logging_level='Silent' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with Simple Neural Net\n",
    "- Add Target Balance is important. \n",
    "- Use ROC_AUC to evaluated the final performance\n",
    "- Use Output CLipping due to it is a binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kent\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from contextlib import contextmanager\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "import keras as ks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def fit_predict(xs, y_train) -> np.ndarray:\n",
    "    X_train, X_test = xs\n",
    "    config = tf.ConfigProto(\n",
    "        intra_op_parallelism_threads=1, use_per_session_threads=1, inter_op_parallelism_threads=1)\n",
    "    with tf.Session(graph=tf.Graph(), config=config) as sess:\n",
    "        ks.backend.set_session(sess)\n",
    "        model_in = ks.Input(shape=(X_train.shape[1],), dtype='float32', sparse=True)\n",
    "        out = ks.layers.Dense(204, activation='relu')(model_in)\n",
    "        out = ks.layers.Dense(68, activation='relu')(out)\n",
    "        out = ks.layers.Dense(68, activation='relu')(out)\n",
    "        out = ks.layers.Dense(1)(out)\n",
    "        model = ks.Model(model_in, out)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=ks.optimizers.Adam(lr=3e-3), metrics=['accuracy'])\n",
    "        for i in range(3):\n",
    "            with timer(f'epoch {i + 1}'):\n",
    "                model.fit(x=X_train, y=y_train, batch_size=2**(11 + i), epochs=1, verbose=0)\n",
    "        return model.predict(X_test)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a6ebdf0aa3c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mRocAucEvaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minterval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Callback' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.interval = interval\n",
    "        self.x_val,self.y_val = validation_data\n",
    "    def on_epoch_end(self, epoch, log={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.x_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print('\\n ROC_AUC - epoch:%d - score:%.6f \\n' % (epoch+1, score))\n",
    "\n",
    "\n",
    "\n",
    "def binary_crossentropy_with_ranking(y_true, y_pred):\n",
    "    \"\"\" Trying to combine ranking loss with numeric precision\"\"\"\n",
    "    # first get the log loss like normal\n",
    "    logloss = K.mean(K.binary_crossentropy(y_pred, y_true), axis=-1)\n",
    "    \n",
    "    # next, build a rank loss\n",
    "    \n",
    "    # clip the probabilities to keep stability\n",
    "    y_pred_clipped = K.clip(y_pred, K.epsilon(), 1-K.epsilon())\n",
    "\n",
    "    # translate into the raw scores before the logit\n",
    "    y_pred_score = K.log(y_pred_clipped / (1 - y_pred_clipped))\n",
    "\n",
    "    # determine what the maximum score for a zero outcome is\n",
    "    y_pred_score_zerooutcome_max = K.max(y_pred_score * (y_true <1))\n",
    "\n",
    "    # determine how much each score is above or below it\n",
    "    rankloss = y_pred_score - y_pred_score_zerooutcome_max\n",
    "\n",
    "    # only keep losses for positive outcomes\n",
    "    rankloss = rankloss * y_true\n",
    "\n",
    "    # only keep losses where the score is below the max\n",
    "    rankloss = K.square(K.clip(rankloss, -100, 0))\n",
    "\n",
    "    # average the loss for just the positive outcomes\n",
    "    rankloss = K.sum(rankloss, axis=-1) / (K.sum(y_true > 0) + 1)\n",
    "\n",
    "    # return (rankloss + 1) * logloss - an alternative to try\n",
    "    return rankloss + logloss\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# AUC for a binary classifier\n",
    "def auc(y_true, y_pred):   \n",
    "    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n",
    "    binSizes = -(pfas[1:]-pfas[:-1])\n",
    "    s = ptas*binSizes\n",
    "    return K.sum(s, axis=0)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# PFA, prob false alert for binary classifier\n",
    "def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # N = total number of negative labels\n",
    "    N = K.sum(1 - y_true)\n",
    "    # FP = total number of false alerts, alerts from the negative class labels\n",
    "    FP = K.sum(y_pred - y_pred * y_true)    \n",
    "    return FP/N\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# P_TA prob true alerts for binary classifier\n",
    "def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # P = total number of positive labels\n",
    "    P = K.sum(y_true)\n",
    "    # TP = total number of correct alerts, alerts from the positive class labels\n",
    "    TP = K.sum(y_pred * y_true)    \n",
    "    return TP/P\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kent\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 641714 samples, validate on 320858 samples\n",
      "Epoch 1/3\n",
      "641714/641714 [==============================] - 44s 68us/step - loss: 0.6036 - acc: 0.6917 - val_loss: 0.5407 - val_acc: 0.7255\n",
      "\n",
      " ROC_AUC - epoch:1 - score:0.793961 \n",
      "\n",
      "Epoch 2/3\n",
      "641714/641714 [==============================] - 43s 66us/step - loss: 0.5409 - acc: 0.7267 - val_loss: 0.5294 - val_acc: 0.7334\n",
      "\n",
      " ROC_AUC - epoch:2 - score:0.804392 \n",
      "\n",
      "Epoch 3/3\n",
      "641714/641714 [==============================] - 42s 65us/step - loss: 0.5222 - acc: 0.7383 - val_loss: 0.5026 - val_acc: 0.7491\n",
      "\n",
      " ROC_AUC - epoch:3 - score:0.827117 \n",
      "\n",
      "Train on 641714 samples, validate on 320858 samples\n",
      "Epoch 1/3\n",
      "641714/641714 [==============================] - 38s 59us/step - loss: 0.4900 - acc: 0.7597 - val_loss: 0.4774 - val_acc: 0.7651\n",
      "\n",
      " ROC_AUC - epoch:1 - score:0.844249 \n",
      "\n",
      "Epoch 2/3\n",
      "641714/641714 [==============================] - 39s 60us/step - loss: 0.4697 - acc: 0.7740 - val_loss: 0.4664 - val_acc: 0.7802\n",
      "\n",
      " ROC_AUC - epoch:2 - score:0.857819 \n",
      "\n",
      "Epoch 3/3\n",
      "641714/641714 [==============================] - 38s 60us/step - loss: 0.4478 - acc: 0.7880 - val_loss: 0.4418 - val_acc: 0.7949\n",
      "\n",
      " ROC_AUC - epoch:3 - score:0.873103 \n",
      "\n",
      "Train on 641714 samples, validate on 320858 samples\n",
      "Epoch 1/3\n",
      "641714/641714 [==============================] - 36s 56us/step - loss: 0.4210 - acc: 0.8049 - val_loss: 0.4498 - val_acc: 0.7990\n",
      "\n",
      " ROC_AUC - epoch:1 - score:0.875005 \n",
      "\n",
      "Epoch 2/3\n",
      "641714/641714 [==============================] - 36s 56us/step - loss: 0.4118 - acc: 0.8119 - val_loss: 0.4236 - val_acc: 0.8134\n",
      "\n",
      " ROC_AUC - epoch:2 - score:0.890217 \n",
      "\n",
      "Epoch 3/3\n",
      "641714/641714 [==============================] - 36s 56us/step - loss: 0.4214 - acc: 0.8027 - val_loss: 0.4125 - val_acc: 0.8083\n",
      "\n",
      " ROC_AUC - epoch:3 - score:0.887806 \n",
      "\n",
      "Train on 641715 samples, validate on 320857 samples\n",
      "Epoch 1/3\n",
      "641715/641715 [==============================] - 46s 72us/step - loss: 0.6007 - acc: 0.6901 - val_loss: 0.5675 - val_acc: 0.7216\n",
      "\n",
      " ROC_AUC - epoch:1 - score:0.792377 \n",
      "\n",
      "Epoch 2/3\n",
      "641715/641715 [==============================] - 44s 69us/step - loss: 0.5373 - acc: 0.7284 - val_loss: 0.5380 - val_acc: 0.7352\n",
      "\n",
      " ROC_AUC - epoch:2 - score:0.808776 \n",
      "\n",
      "Epoch 3/3\n",
      "641715/641715 [==============================] - 44s 69us/step - loss: 0.5238 - acc: 0.7371 - val_loss: 0.5321 - val_acc: 0.7470\n",
      "\n",
      " ROC_AUC - epoch:3 - score:0.823922 \n",
      "\n",
      "Train on 641715 samples, validate on 320857 samples\n",
      "Epoch 1/3\n",
      "641715/641715 [==============================] - 43s 67us/step - loss: 0.4845 - acc: 0.7639 - val_loss: 0.5579 - val_acc: 0.7693\n",
      "\n",
      " ROC_AUC - epoch:1 - score:0.849574 \n",
      "\n",
      "Epoch 2/3\n",
      "641715/641715 [==============================] - 48s 76us/step - loss: 0.4828 - acc: 0.7665 - val_loss: 0.5587 - val_acc: 0.7386\n",
      "\n",
      " ROC_AUC - epoch:2 - score:0.809745 \n",
      "\n",
      "Epoch 3/3\n",
      "641715/641715 [==============================] - 48s 75us/step - loss: 0.4766 - acc: 0.7700 - val_loss: 0.5283 - val_acc: 0.7718\n"
     ]
    }
   ],
   "source": [
    "def fit_predict(df,  feats, num_folds=3, stratified=True, test=None):\n",
    "    for col in  ['INS_PAYMENT_PERC_MAX', 'INS_PAYMENT_PERC_MEAN', 'INS_PAYMENT_PERC_SUM', \n",
    "                 'PREV_APP_CREDIT_PERC_MAX', 'PREV_APP_CREDIT_PERC_MEAN', \n",
    "                 'REF_APP_CREDIT_PERC_MAX', 'REF_APP_CREDIT_PERC_MEAN']:\n",
    "        df[col] = df[col].apply(lambda x: x/(1.+abs(x)) if x < 100000 else 1.1)\n",
    "        \n",
    "    df = df.append(df, ignore_index=True)\n",
    "    \n",
    "    agent = PandasStandardScaler()\n",
    "    agent.fit(df[feats])\n",
    "    \n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    \n",
    "    del df \n",
    "    gc.collect()\n",
    "    \n",
    "    train_df = agent.transform(train_df)\n",
    "    test_df = agent.transform(test_df)\n",
    "    \n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "\n",
    "    \n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=32)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "        \n",
    "        config = tf.ConfigProto(\n",
    "            intra_op_parallelism_threads=1, use_per_session_threads=1, inter_op_parallelism_threads=1)\n",
    "        with tf.Session(graph=tf.Graph(), config=config) as sess:\n",
    "            ks.backend.set_session(sess)\n",
    "            model_in = ks.Input(shape=(train_x.shape[1],), dtype='float32')\n",
    "            out = ks.layers.Dense(128, activation='relu')(model_in)\n",
    "            out = ks.layers.Dense(64, activation='relu')(out)\n",
    "            out = ks.layers.Dropout(0.2)(out)\n",
    "            out = Lambda(lambda x: clip(x, min_value=0, max_value=1))(out)\n",
    "            out = ks.layers.Dense(32, activation='relu')(out)\n",
    "            out = ks.layers.Dense(1)(out)\n",
    "            out = Lambda(lambda x: clip(x, min_value=0, max_value=1))(out)\n",
    "            model = ks.Model(model_in, out)\n",
    "            model.compile(loss='binary_crossentropy', \n",
    "                          optimizer=ks.optimizers.Adam(lr=1e-3), \n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "            RocAuc = RocAucEvaluation(validation_data=(valid_x, valid_y), interval=1)\n",
    "            \n",
    "            for i in range(3):\n",
    "                model.fit(x=train_x, y=train_y, batch_size=200+i*48, \n",
    "                              epochs=3, verbose=1, \n",
    "                              validation_data=(valid_x, valid_y), callbacks=[RocAuc], \n",
    "                             )\n",
    "            pre_test = model.predict(test_df[feats])[:, 0]\n",
    " \n",
    "        sub_preds += pre_test / folds.n_splits\n",
    "        if test:\n",
    "            print('test')\n",
    "            break\n",
    "        #model.predict(X_test)[:, 0]\n",
    "        \n",
    "    submission_file_name = 'simple_dl_4.csv'\n",
    "    test_df['TARGET'] = sub_preds\n",
    "    test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)\n",
    "\n",
    "    print('saved ', submission_file_name)\n",
    "\n",
    "feats = [f for f in df_fill.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV', 'index', 'index_x', 'index_y']]\n",
    "\n",
    "fit_predict(df_fill, feats=feats,  stratified=True, test=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |         C |     gamma | \n",
      "    1 | 00m00s | \u001b[35m   0.74473\u001b[0m | \u001b[32m   0.0010\u001b[0m | \u001b[32m   0.0010\u001b[0m | \n",
      "    2 | 00m00s | \u001b[35m   0.80703\u001b[0m | \u001b[32m   0.0100\u001b[0m | \u001b[32m   0.0100\u001b[0m | \n",
      "    3 | 00m00s |    0.67984 |    0.1000 |    0.1000 | \n",
      "    4 | 00m00s | \u001b[35m   0.85934\u001b[0m | \u001b[32m  29.8991\u001b[0m | \u001b[32m   0.0396\u001b[0m | \n",
      "    5 | 00m00s |    0.75949 |   83.7709 |    0.0965 | \n",
      "    6 | 00m00s |    0.85730 |   16.7875 |    0.0401 | \n",
      "    7 | 00m00s | \u001b[35m   0.92209\u001b[0m | \u001b[32m   8.4303\u001b[0m | \u001b[32m   0.0138\u001b[0m | \n",
      "    8 | 00m00s |    0.77421 |   74.4476 |    0.0851 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |         C |     gamma | \n",
      "    9 | 00m00s |    0.85730 |    8.1330 |    0.0401 | \n",
      "   10 | 00m00s |    0.83861 |   29.5721 |    0.0487 | \n",
      "   11 | 00m00s |    0.78944 |   16.4470 |    0.0748 | \n",
      "   12 | 00m00s |    0.77617 |   74.8205 |    0.0833 | \n",
      "   13 | 00m01s |    0.76608 |   84.1584 |    0.0903 | \n",
      "   14 | 00m01s |    0.75564 |    8.8226 |    0.1000 | \n",
      "   15 | 00m00s |    0.75564 |   17.2052 |    0.1000 | \n",
      "   16 | 00m01s |    0.75564 |   30.3338 |    0.1000 | \n",
      "   17 | 00m01s |    0.83944 |   29.1234 |    0.0001 | \n",
      "   18 | 00m01s |    0.75564 |   28.6969 |    0.1000 | \n",
      "-----------------------------------------------------\n",
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   max_features |   min_samples_split |   n_estimators | \n",
      "    1 | 00m01s | \u001b[35m   0.82251\u001b[0m | \u001b[32m        0.4906\u001b[0m | \u001b[32m            16.0376\u001b[0m | \u001b[32m      186.5575\u001b[0m | \n",
      "    2 | 00m01s | \u001b[35m   0.82742\u001b[0m | \u001b[32m        0.5317\u001b[0m | \u001b[32m             9.5087\u001b[0m | \u001b[32m      151.5355\u001b[0m | \n",
      "    3 | 00m01s |    0.82022 |         0.6758 |             14.6900 |       126.0662 | \n",
      "    4 | 00m00s |    0.79904 |         0.1937 |             14.4711 |        27.8007 | \n",
      "    5 | 00m02s |    0.80626 |         0.7509 |             20.2507 |       189.1292 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   max_features |   min_samples_split |   n_estimators | \n",
      "    6 | 00m09s |    0.81705 |         0.8602 |              2.0038 |       249.8697 | \n",
      "    7 | 00m07s | \u001b[35m   0.86132\u001b[0m | \u001b[32m        0.1000\u001b[0m | \u001b[32m             2.0000\u001b[0m | \u001b[32m      136.4631\u001b[0m | \n",
      "    8 | 00m05s |    0.82351 |         0.7877 |              2.0022 |        68.8049 | \n",
      "    9 | 00m04s |    0.73826 |         0.1529 |              2.0303 |        10.1821 | \n",
      "   10 | 00m06s |    0.85821 |         0.1000 |              2.0000 |       207.2543 | \n",
      "   11 | 00m08s |    0.81728 |         0.9483 |              2.0427 |       181.9460 | \n",
      "   12 | 00m06s |    0.83698 |         0.1941 |             24.8502 |       249.5687 | \n",
      "   13 | 00m05s |    0.80248 |         0.2301 |             24.9792 |        10.1612 | \n",
      "   14 | 00m06s |    0.83363 |         0.1006 |             15.1933 |       226.4367 | \n",
      "   15 | 00m05s |    0.83509 |         0.1000 |             25.0000 |        69.1014 | \n",
      "-----------------------------------------------------\n",
      "Final Results\n",
      "SVC: 0.922088\n",
      "RFC: 0.861324\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Load data set and target values\n",
    "data, target = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=45,\n",
    "    n_informative=12,\n",
    "    n_redundant=7\n",
    ")\n",
    "\n",
    "def svccv(C, gamma):\n",
    "    val = cross_val_score(\n",
    "        SVC(C=C, gamma=gamma, random_state=2),\n",
    "        data, target, 'f1', cv=2\n",
    "    ).mean()\n",
    "\n",
    "    return val\n",
    "\n",
    "def rfccv(n_estimators, min_samples_split, max_features):\n",
    "    val = cross_val_score(\n",
    "        RFC(n_estimators=int(n_estimators),\n",
    "            min_samples_split=int(min_samples_split),\n",
    "            max_features=min(max_features, 0.999),\n",
    "            random_state=2\n",
    "        ),\n",
    "        data, target, 'f1', cv=2\n",
    "    ).mean()\n",
    "    return val\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gp_params = {\"alpha\": 1e-5}\n",
    "\n",
    "    svcBO = BayesianOptimization(svccv,\n",
    "        {'C': (0.001, 100), 'gamma': (0.0001, 0.1)})\n",
    "    svcBO.explore({'C': [0.001, 0.01, 0.1], 'gamma': [0.001, 0.01, 0.1]})\n",
    "\n",
    "    rfcBO = BayesianOptimization(\n",
    "        rfccv,\n",
    "        {'n_estimators': (10, 250),\n",
    "        'min_samples_split': (2, 25),\n",
    "        'max_features': (0.1, 0.999)}\n",
    "    )\n",
    "\n",
    "    svcBO.maximize(n_iter=10, **gp_params)\n",
    "    print('-' * 53)\n",
    "    rfcBO.maximize(n_iter=10, **gp_params)\n",
    "\n",
    "    print('-' * 53)\n",
    "    print('Final Results')\n",
    "    print('SVC: %f' % svcBO.res['max']['max_val'])\n",
    "    print('RFC: %f' % rfcBO.res['max']['max_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
